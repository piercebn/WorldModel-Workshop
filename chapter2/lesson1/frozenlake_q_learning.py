import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import sys
import os

# ==================== å¸¸é‡å®šä¹‰ ====================
class QLearningConfig:
    """Q-Learningç®—æ³•é…ç½®å¸¸é‡"""
    # ç¯å¢ƒé…ç½®
    ENV_NAME = 'FrozenLake-v1'
    MAP_NAME = "8x8"  # æ”¹ä¸º8x8åœ°å›¾
    IS_SLIPPERY = False  # å…³é—­æ»‘å†°ï¼Œè®©æ™ºèƒ½ä½“å­¦ä¼šåŸºæœ¬ç­–ç•¥
    
    # å­¦ä¹ å‚æ•°
    DEFAULT_LEARNING_RATE = 0.9
    DEFAULT_EPSILON = 1.0
    DEFAULT_DISCOUNT_FACTOR = 0.9
    DEFAULT_EPSILON_DECAY_RATE = 0.00001  # è¿›ä¸€æ­¥å‡æ…¢æ¢ç´¢ç‡è¡°å‡
    
    # å¥–åŠ±å¡‘é€ 
    HOLE_PENALTY = -0.1
    SUCCESS_REWARD = 1.0
    PROGRESS_REWARD = 0.01  # å‘ç›®æ ‡é è¿‘çš„å¥–åŠ±
    STEP_PENALTY = -0.001   # æ¯æ­¥çš„å°æƒ©ç½šï¼Œé¼“åŠ±å°½å¿«åˆ°è¾¾ç›®æ ‡
    
    # è®­ç»ƒé…ç½®
    DEFAULT_TRAINING_EPISODES = 50000  # 8x8ç¯å¢ƒéœ€è¦æ›´å¤šè®­ç»ƒå›åˆ
    DEMO_EPISODES = 100
    
    # æ£€æŸ¥ç‚¹é…ç½®
    CHECKPOINT_INTERVAL_MAJOR = 20  # æ¯5%ä¿å­˜ä¸€æ¬¡
    CHECKPOINT_INTERVAL_AUTO = 1000  # æ¯1000å›åˆè‡ªåŠ¨ä¿å­˜
    CHECKPOINT_INTERVAL_QUICK = 100  # æ¯100å›åˆå¿«é€Ÿä¿å­˜
    
    # æ—¥å¿—é…ç½®
    LOG_INTERVAL_PERCENT = 100  # æ¯1%è¾“å‡ºä¸€æ¬¡æ—¥å¿—
    DETAIL_LOG_STEPS = 15  # è¯¦ç»†æ—¥å¿—æ˜¾ç¤ºæ­¥æ•°
    DETAIL_LOG_EARLY_EPISODES = 5  # å‰å‡ å›åˆæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    
    # è¿›åº¦æ¡é…ç½®
    PROGRESS_BAR_LENGTH = 30
    
    # å­¦ä¹ ç‡è°ƒæ•´
    EPSILON_THRESHOLD = 0.01
    LEARNING_RATE_DECAY = 0.9999
    MIN_LEARNING_RATE = 0.01  # ä¿æŒ1%çš„æœ€å°å­¦ä¹ ç‡ï¼Œä¸è¦è¿‡åº¦è¡°å‡
    MIN_EPSILON = 0.2  # ä¿æŒ20%çš„æœ€å°æ¢ç´¢ç‡ï¼Œå¢åŠ æ¢ç´¢
    
    # å›¾è¡¨é…ç½®
    PLOT_FIGURE_SIZE = (12, 8)
    PLOT_DPI = 150
    MOVING_AVERAGE_WINDOW = 100
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    ENABLE_PERFORMANCE_OPTIMIZATION = True
    CACHE_Q_MAX = True  # ç¼“å­˜Qå€¼æœ€å¤§å€¼è®¡ç®—
    
    # æ–‡ä»¶é…ç½®
    MODEL_FILE = 'frozen_lake8x8.pkl'
    CHECKPOINT_FILE = 'frozenlake_checkpoint.pkl'
    PLOT_FILE = 'frozen_lake8x8.png'
    
    # ç¯å¢ƒçŠ¶æ€é…ç½®
    MAX_ACTIONS_PER_EPISODE = 200
    MAP_SIZE = 8  # 8x8åœ°å›¾
    
    # åŠ¨ä½œåç§°
    ACTION_NAMES = ["å·¦", "ä¸‹", "å³", "ä¸Š"]
    
    # éªŒè¯é…ç½®
    MIN_EPSILON = 0.0
    MAX_EPSILON = 1.0
    MIN_LEARNING_RATE_VALID = 0.0
    MAX_LEARNING_RATE_VALID = 1.0
    
    # æ˜¾ç¤ºé…ç½®
    STRATEGY_DISPLAY_ROWS = 4
    STRATEGY_DISPLAY_COLS = 4
    SEPARATOR_LENGTH = 80
    DEMO_SEPARATOR_LENGTH = 50
    LOG_SEPARATOR_LENGTH = 70

# ==================== è¾…åŠ©å‡½æ•° ====================
def validate_parameters(episodes):
    """éªŒè¯è¾“å…¥å‚æ•°"""
    if episodes <= 0:
        print("âŒ å›åˆæ•°å¿…é¡»å¤§äº0")
        return False
    
    if not isinstance(episodes, int):
        print("âŒ å›åˆæ•°å¿…é¡»æ˜¯æ•´æ•°")
        return False
    
    return True

def create_environment(render=False):
    """åˆ›å»ºFrozenLakeç¯å¢ƒ"""
    try:
        env = gym.make(
            QLearningConfig.ENV_NAME, 
            map_name=QLearningConfig.MAP_NAME, 
            is_slippery=QLearningConfig.IS_SLIPPERY, 
            render_mode='human' if render else None
        )
        return env
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…gymnasiumå’ŒFrozenLakeç¯å¢ƒ")
        return None

def load_checkpoint(checkpoint_file, env):
    """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    try:
        if not os.path.isfile(checkpoint_file):
            print(f"âš ï¸  checkpointè·¯å¾„ä¸æ˜¯æ–‡ä»¶: {checkpoint_file}")
            raise FileNotFoundError("Invalid checkpoint path")
            
        with open(checkpoint_file, 'rb') as f:
            checkpoint = pickle.load(f)
            
        # éªŒè¯checkpointæ•°æ®ç»“æ„
        required_keys = ['q_table', 'episode', 'epsilon', 'learning_rate']
        if not all(key in checkpoint for key in required_keys):
            print(f"âš ï¸  checkpointæ–‡ä»¶æ ¼å¼æ— æ•ˆï¼Œç¼ºå°‘å¿…è¦å­—æ®µ")
            raise ValueError("Invalid checkpoint format")
            
        q = checkpoint['q_table']
        start_episode = checkpoint['episode']
        epsilon = checkpoint['epsilon']
        learning_rate_a = checkpoint['learning_rate']
        
        # éªŒè¯Qè¡¨ç»´åº¦æ˜¯å¦ä¸ç¯å¢ƒåŒ¹é…
        expected_shape = (env.observation_space.n, env.action_space.n)
        if q.shape != expected_shape:
            print(f"âš ï¸  Qè¡¨ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_shape}, å®é™…{q.shape}")
            raise ValueError("Q table dimension mismatch")
        
        # éªŒè¯å‚æ•°å€¼çš„åˆç†æ€§
        if not (QLearningConfig.MIN_EPSILON <= epsilon <= QLearningConfig.MAX_EPSILON):
            print(f"âš ï¸  epsilonå€¼ä¸åˆç†: {epsilon}, é‡ç½®ä¸º{QLearningConfig.DEFAULT_EPSILON}")
            epsilon = QLearningConfig.DEFAULT_EPSILON
        if not (QLearningConfig.MIN_LEARNING_RATE_VALID < learning_rate_a <= QLearningConfig.MAX_LEARNING_RATE_VALID):
            print(f"âš ï¸  å­¦ä¹ ç‡å€¼ä¸åˆç†: {learning_rate_a}, é‡ç½®ä¸º{QLearningConfig.DEFAULT_LEARNING_RATE}")
            learning_rate_a = QLearningConfig.DEFAULT_LEARNING_RATE
        if start_episode < 0:
            print(f"âš ï¸  å›åˆæ•°ä¸åˆç†: {start_episode}, é‡ç½®ä¸º0")
            start_episode = 0
        
        print(f"âœ… åŠ è½½checkpoint: ä»ç¬¬{start_episode}å›åˆç»§ç»­è®­ç»ƒ")
        print(f"   å½“å‰Qè¡¨çŠ¶æ€: å¹³å‡Qå€¼={np.mean(np.max(q, axis=1)):.4f}")
        return q, start_episode, epsilon, learning_rate_a
        
    except Exception as e:
        print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}, ä»å¤´å¼€å§‹è®­ç»ƒ")
        return None, 0, QLearningConfig.DEFAULT_EPSILON, QLearningConfig.DEFAULT_LEARNING_RATE

def save_checkpoint(q, episode, epsilon, learning_rate, success_rate, avg_q, checkpoint_file):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    try:
        checkpoint = {
            'q_table': q,
            'episode': episode,
            'epsilon': epsilon,
            'learning_rate': learning_rate,
            'success_rate': success_rate,
            'avg_q': avg_q
        }
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(checkpoint, f)
        return True
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜checkpointå¤±è´¥: {e}")
        return False

def run(episodes, is_training=True, render=False, log_details=True, checkpoint_file=None):
    """
    è¿è¡ŒQ-Learningç®—æ³•
    
    Args:
        episodes: è®­ç»ƒå›åˆæ•°
        is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        render: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–ç•Œé¢
        log_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºé…ç½®ä¸­çš„é»˜è®¤å€¼
    """
    # ä½¿ç”¨é»˜è®¤æ£€æŸ¥ç‚¹æ–‡ä»¶
    if checkpoint_file is None:
        checkpoint_file = QLearningConfig.CHECKPOINT_FILE
    
    # å‚æ•°éªŒè¯
    if not validate_parameters(episodes):
        return

    # åˆ›å»ºç¯å¢ƒ
    env = create_environment(render)
    if env is None:
        return

    # åˆå§‹åŒ–Qè¡¨å’Œå‚æ•°
    if is_training:
        if os.path.exists(checkpoint_file):
            q, start_episode, epsilon, learning_rate_a = load_checkpoint(checkpoint_file, env)
            if q is None:  # åŠ è½½å¤±è´¥ï¼Œé‡æ–°åˆå§‹åŒ–
                # ä½¿ç”¨å°çš„éšæœºå€¼åˆå§‹åŒ–Qè¡¨ï¼Œè€Œä¸æ˜¯å…¨é›¶
                q = np.random.uniform(-0.1, 0.1, (env.observation_space.n, env.action_space.n))
        else:
            # ä½¿ç”¨å°çš„éšæœºå€¼åˆå§‹åŒ–Qè¡¨ï¼Œè€Œä¸æ˜¯å…¨é›¶
            q = np.random.uniform(-0.1, 0.1, (env.observation_space.n, env.action_space.n))
            start_episode = 0
            epsilon = QLearningConfig.DEFAULT_EPSILON
            learning_rate_a = QLearningConfig.DEFAULT_LEARNING_RATE
    else:
        # æµ‹è¯•æ¨¡å¼ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        try:
            with open(QLearningConfig.MODEL_FILE, 'rb') as f:
                q = pickle.load(f)
            # æµ‹è¯•æ¨¡å¼ä¸‹ä¹Ÿéœ€è¦åˆå§‹åŒ–epsilon
            epsilon = 0.0
            learning_rate_a = QLearningConfig.DEFAULT_LEARNING_RATE
            start_episode = 0
        except FileNotFoundError:
            print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ '{QLearningConfig.MODEL_FILE}'")
            print("è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼")
            return

    discount_factor_g = QLearningConfig.DEFAULT_DISCOUNT_FACTOR  # gamma or discount rate
    epsilon_decay_rate = QLearningConfig.DEFAULT_EPSILON_DECAY_RATE  # epsilon decay rate
    rng = np.random.default_rng()   # random number generator

    rewards_per_episode = np.zeros(episodes)
    
    # æ—¥å¿—ç›¸å…³å˜é‡
    action_names = QLearningConfig.ACTION_NAMES
    log_interval = max(1, episodes // QLearningConfig.LOG_INTERVAL_PERCENT)  # æ¯1%çš„è¿›åº¦è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ—¥å¿—
    step_count = 0
    
    if log_details and is_training:
        print("=" * QLearningConfig.SEPARATOR_LENGTH)
        print("ğŸ¯ Q-Learning ç®—æ³•è®­ç»ƒå¼€å§‹")
        print(f"ğŸ“Š ç¯å¢ƒ: FrozenLake 8x8, æ€»å›åˆæ•°: {episodes}")
        print(f"ğŸ§  åˆå§‹å‚æ•°: Î±(å­¦ä¹ ç‡)={learning_rate_a}, Î³(æŠ˜æ‰£å› å­)={discount_factor_g}, Îµ(æ¢ç´¢ç‡)={epsilon}")
        print("=" * QLearningConfig.SEPARATOR_LENGTH)

    for i in range(episodes):
        # æ˜¾ç¤ºè¿›åº¦æ¡
        if not log_details and is_training:
            # è®¡ç®—æ€»ä½“è¿›åº¦ï¼ˆåŒ…æ‹¬ä¹‹å‰è®­ç»ƒçš„å›åˆæ•°ï¼‰
            total_episodes_trained = start_episode + i + 1
            total_target_episodes = start_episode + episodes  # ä½¿ç”¨å®é™…ç›®æ ‡å›åˆæ•°
            progress = total_episodes_trained / total_target_episodes * 100
            bar_length = QLearningConfig.PROGRESS_BAR_LENGTH
            filled_length = int(bar_length * progress / 100)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            print(f'\rğŸš€ è®­ç»ƒè¿›åº¦: [{bar}] {progress:.1f}% ({total_episodes_trained}/{total_target_episodes})', end='', flush=True)
        
        state = env.reset()[0]  # states: 0 to 63, 0=top left corner,63=bottom right corner
        terminated = False      # True when fall in hole or reached goal
        truncated = False       # True when actions > MAX_ACTIONS_PER_EPISODE
        episode_steps = 0
        episode_reward = 0

        # è®°å½•å›åˆå¼€å§‹ä¿¡æ¯
        show_episode_detail = log_details and is_training and (i % log_interval == 0 or i < QLearningConfig.DETAIL_LOG_EARLY_EPISODES)
        
        if show_episode_detail:
            print(f"\nğŸ“ å›åˆ {i+1}/{episodes} (è¿›åº¦: {(i+1)/episodes*100:.1f}%)")
            print(f"ğŸ² å½“å‰æ¢ç´¢ç‡ Îµ = {epsilon:.4f}, å­¦ä¹ ç‡ Î± = {learning_rate_a:.4f}")
            print(f"ğŸ—ºï¸  èµ·å§‹ä½ç½®: çŠ¶æ€ {state} (ä½ç½®: è¡Œ{state//QLearningConfig.MAP_SIZE}, åˆ—{state%QLearningConfig.MAP_SIZE})")

        while(not terminated and not truncated):
            step_count += 1
            episode_steps += 1
            
            # è¯¦ç»†æ—¥å¿— - æ­¥éª¤å¼€å§‹
            if show_episode_detail and episode_steps <= QLearningConfig.DETAIL_LOG_STEPS:
                print(f"\n  ğŸ“ æ­¥éª¤ {episode_steps}: å½“å‰çŠ¶æ€ {state} (è¡Œ{state//QLearningConfig.MAP_SIZE}, åˆ—{state%QLearningConfig.MAP_SIZE})")
                print(f"     å½“å‰çŠ¶æ€Qå€¼: {q[state,:]}")
                sys.stdout.flush()
            
            # åŠ¨ä½œé€‰æ‹©ç­–ç•¥
            random_value = rng.random() if is_training else 0
            if is_training and random_value < epsilon:
                action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up
                action_type = "ğŸ² éšæœºæ¢ç´¢"
                if show_episode_detail and episode_steps <= QLearningConfig.DETAIL_LOG_STEPS:
                    print(f"     å†³ç­–ä¾æ®: éšæœºæ•° {random_value:.4f} < Îµ({epsilon:.4f}) â†’ éšæœºé€‰æ‹©")
                    sys.stdout.flush()
            else:
                action = np.argmax(q[state,:])
                action_type = "ğŸ§  ç­–ç•¥é€‰æ‹©"
                if show_episode_detail and episode_steps <= QLearningConfig.DETAIL_LOG_STEPS:
                    best_q_values = q[state,:]
                    print(f"     å†³ç­–ä¾æ®: é€‰æ‹©æœ€å¤§Qå€¼åŠ¨ä½œ")
                    for act_idx, (act_name, q_val) in enumerate(zip(action_names, best_q_values)):
                        marker = " âœ…" if act_idx == action else ""
                        print(f"       åŠ¨ä½œ'{act_name}': Q={q_val:.4f}{marker}")
                    sys.stdout.flush()
                
            # è®°å½•åŠ¨ä½œå‰çš„Qå€¼
            old_q_value = q[state, action] if is_training else 0
            
            if show_episode_detail and episode_steps <= QLearningConfig.DETAIL_LOG_STEPS:
                print(f"     â¡ï¸  é€‰æ‹©åŠ¨ä½œ: '{action_names[action]}' ({action_type})")
                sys.stdout.flush()

            new_state,reward,terminated,truncated,_ = env.step(action)
            episode_reward += reward

            if is_training:
                # ğŸ¯ å¥–åŠ±å¡‘é€ : å¤šå±‚æ¬¡çš„å¥–åŠ±ç³»ç»Ÿ
                shaped_reward = reward  # åŸºç¡€å¥–åŠ±
                
                if terminated and reward == 0.0:  # æ‰å…¥å†°æ´
                    shaped_reward += QLearningConfig.HOLE_PENALTY  # è´Ÿå¥–åŠ±æƒ©ç½šæ‰å…¥å†°æ´
                elif not terminated:  # æœªç»“æŸï¼Œç»™äºˆæ­¥æ•°æƒ©ç½š
                    shaped_reward += QLearningConfig.STEP_PENALTY  # æ¯æ­¥å°æƒ©ç½šï¼Œé¼“åŠ±å°½å¿«åˆ°è¾¾
                
                # è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦»å¥–åŠ±ï¼ˆå¯é€‰ï¼‰
                # è¿™é‡Œå¯ä»¥æ·»åŠ åŸºäºè·ç¦»çš„å¥–åŠ±ï¼Œä½†éœ€è¦çŸ¥é“ç›®æ ‡ä½ç½®
                
                # Q-learning æ›´æ–°å…¬å¼è¯¦ç»†è¿‡ç¨‹
                if QLearningConfig.CACHE_Q_MAX and hasattr(env, '_cached_q_max') and env._cached_q_max is not None:
                    max_next_q = env._cached_q_max[new_state]
                else:
                    max_next_q = np.max(q[new_state,:])
                    if QLearningConfig.CACHE_Q_MAX:
                        if not hasattr(env, '_cached_q_max'):
                            env._cached_q_max = np.zeros(env.observation_space.n)
                        env._cached_q_max[new_state] = max_next_q
                
                target = shaped_reward + discount_factor_g * max_next_q
                td_error = target - old_q_value
                new_q_value = old_q_value + learning_rate_a * td_error
                q[state,action] = new_q_value
                
                # æ›´æ–°ç¼“å­˜
                if QLearningConfig.CACHE_Q_MAX and hasattr(env, '_cached_q_max'):
                    env._cached_q_max[state] = np.max(q[state,:])
                
                # è¯¦ç»†æ—¥å¿—è¾“å‡º - Qå€¼æ›´æ–°è¿‡ç¨‹
                if show_episode_detail and episode_steps <= QLearningConfig.DETAIL_LOG_STEPS:
                    print(f"     ğŸ”„ çŠ¶æ€è½¬ç§»: {state} â†’ {new_state} (è¡Œ{new_state//QLearningConfig.MAP_SIZE}, åˆ—{new_state%QLearningConfig.MAP_SIZE})")
                    print(f"     ğŸ† ç¯å¢ƒå¥–åŠ±: r = {reward}")
                    if shaped_reward != reward:
                        print(f"     ğŸ¯ å¥–åŠ±å¡‘é€ : shaped_r = {shaped_reward} (æ‰å…¥å†°æ´æƒ©ç½š {QLearningConfig.HOLE_PENALTY})")
                    else:
                        print(f"     ğŸ¯ æœ€ç»ˆå¥–åŠ±: shaped_r = {shaped_reward}")
                    print(f"     ğŸ“Š Q-Learning æ›´æ–°è®¡ç®—è¯¦ç»†è¿‡ç¨‹:")
                    print(f"       ğŸ”¹ æ­¥éª¤1: è·å–å½“å‰Qå€¼")
                    print(f"         Q(s={state}, a={action}) = {old_q_value:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤2: æŸ¥çœ‹æ–°çŠ¶æ€çš„æ‰€æœ‰Qå€¼")
                    print(f"         Q({new_state}, :) = {q[new_state,:]}")
                    print(f"       ğŸ”¹ æ­¥éª¤3: æ‰¾åˆ°æ–°çŠ¶æ€çš„æœ€å¤§Qå€¼")
                    print(f"         max Q(s'={new_state}, a') = {max_next_q:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤4: ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—ç›®æ ‡å€¼")
                    print(f"         Target = shaped_r + Î³ Ã— max Q(s', a')")
                    print(f"         Target = {shaped_reward} + {discount_factor_g} Ã— {max_next_q:.4f}")
                    print(f"         Target = {target:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤5: è®¡ç®—æ—¶åºå·®åˆ†(TD)è¯¯å·®")
                    print(f"         TD_error = Target - Q(s,a)")
                    print(f"         TD_error = {target:.4f} - {old_q_value:.4f}")
                    print(f"         TD_error = {td_error:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤6: ä½¿ç”¨å­¦ä¹ ç‡æ›´æ–°Qå€¼")
                    print(f"         Q_new(s,a) = Q_old(s,a) + Î± Ã— TD_error")
                    print(f"         Q_new({state},{action}) = {old_q_value:.4f} + {learning_rate_a} Ã— {td_error:.4f}")
                    print(f"         Q_new({state},{action}) = {new_q_value:.4f}")
                    print(f"     ğŸ“‹ æ›´æ–°åçš„Qè¡¨çŠ¶æ€:")
                    print(f"       å½“å‰çŠ¶æ€Qå€¼: Q({state},:) = {q[state,:]}")
                    
                    if terminated:
                        if reward > 0:
                            print(f"     ğŸ‰ åˆ°è¾¾ç›®æ ‡! è·å¾—å¥–åŠ± {reward}")
                        else:
                            print(f"     â„ï¸ æ‰å…¥å†°æ´! å›åˆç»“æŸ")
                    
                    print(f"     {'='*QLearningConfig.LOG_SEPARATOR_LENGTH}")
                    # æ¯ä¸ªåŠ¨ä½œæ­¥éª¤åç«‹å³åˆ·æ–°è¾“å‡º
                    sys.stdout.flush()
            else:
                if show_episode_detail and episode_steps <= QLearningConfig.DETAIL_LOG_STEPS:
                    print(f"     ğŸ”„ çŠ¶æ€è½¬ç§»: {state} â†’ {new_state} (æµ‹è¯•æ¨¡å¼ï¼Œä¸æ›´æ–°Qå€¼)")
                    sys.stdout.flush()

            state = new_state

        epsilon = max(epsilon - epsilon_decay_rate, QLearningConfig.MIN_EPSILON)

        # æ”¹è¿›å­¦ä¹ ç‡è°ƒæ•´ï¼šå½“epsilonæ¥è¿‘0æ—¶ï¼Œé€æ¸é™ä½å­¦ä¹ ç‡
        if epsilon < QLearningConfig.EPSILON_THRESHOLD:
            learning_rate_a = max(learning_rate_a * QLearningConfig.LEARNING_RATE_DECAY, QLearningConfig.MIN_LEARNING_RATE)

        if reward == QLearningConfig.SUCCESS_REWARD:
            rewards_per_episode[i] = QLearningConfig.SUCCESS_REWARD
            
        # å›åˆæ€»ç»“
        if show_episode_detail:
            success_rate = np.sum(rewards_per_episode[:i+1]) / (i+1) * 100
            print(f"  ğŸ“‹ å›åˆæ€»ç»“: {episode_steps}æ­¥, å¥–åŠ±={episode_reward}, å½“å‰æˆåŠŸç‡={success_rate:.1f}%")
            
        # è¿›åº¦æ›´æ–°å’Œcheckpointä¿å­˜ - æ¯5%çš„è¿›åº¦ä¿å­˜ä¸€æ¬¡
        if log_details and is_training and i % max(1, episodes // QLearningConfig.CHECKPOINT_INTERVAL_MAJOR) == 0 and i > 0:
            success_rate = np.sum(rewards_per_episode[:i+1]) / (i+1) * 100
            avg_q = np.mean(np.max(q, axis=1))
            print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦ {i+1}/{episodes} ({(i+1)/episodes*100:.0f}%): "
                  f"æˆåŠŸç‡={success_rate:.1f}%, å¹³å‡Qå€¼={avg_q:.3f}, Îµ={epsilon:.4f}")
            
            # ä¿å­˜checkpoint
            try:
                checkpoint = {
                    'q_table': q,
                    'episode': start_episode + i + 1,  # ä¿®æ­£ï¼šä½¿ç”¨å…¨å±€å›åˆæ•°
                    'epsilon': epsilon,
                    'learning_rate': learning_rate_a,
                    'success_rate': success_rate,
                    'avg_q': avg_q
                }
                with open(checkpoint_file, 'wb') as f:
                    pickle.dump(checkpoint, f)
                print(f"ğŸ’¾ å·²ä¿å­˜checkpointåˆ° {checkpoint_file}")
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜checkpointå¤±è´¥: {e}")
            
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        
        # æ¯1000å›åˆè‡ªåŠ¨ä¿å­˜checkpointï¼ˆå³ä½¿æ²¡æœ‰è¯¦ç»†æ—¥å¿—ï¼‰
        elif is_training and i % QLearningConfig.CHECKPOINT_INTERVAL_AUTO == 0 and i > 0:
            try:
                checkpoint = {
                    'q_table': q,
                    'episode': start_episode + i + 1,  # ä¿®æ­£ï¼šä½¿ç”¨å…¨å±€å›åˆæ•°
                    'epsilon': epsilon,
                    'learning_rate': learning_rate_a,
                    'success_rate': np.sum(rewards_per_episode[:i+1]) / (i+1) * 100,
                    'avg_q': np.mean(np.max(q, axis=1))
                }
                with open(checkpoint_file, 'wb') as f:
                    pickle.dump(checkpoint, f)
                if not log_details:
                    print(f"\rğŸ’¾ è‡ªåŠ¨ä¿å­˜checkpointåˆ° {checkpoint_file} (å›åˆ {start_episode + i + 1})", end='', flush=True)
            except Exception as e:
                if not log_details:
                    print(f"\râš ï¸  ä¿å­˜checkpointå¤±è´¥: {e}", end='', flush=True)
        
        # æ¯100å›åˆå¿«é€Ÿä¿å­˜ï¼ˆé˜²æ­¢é¢‘ç¹ä¸­æ–­ï¼‰
        elif is_training and i % QLearningConfig.CHECKPOINT_INTERVAL_QUICK == 0 and i > 0:
            try:
                quick_checkpoint = {
                    'q_table': q,
                    'episode': start_episode + i + 1,  # ä¿®æ­£ï¼šä½¿ç”¨å…¨å±€å›åˆæ•°
                    'epsilon': epsilon,
                    'learning_rate': learning_rate_a,
                    'success_rate': np.sum(rewards_per_episode[:i+1]) / (i+1) * 100,
                    'avg_q': np.mean(np.max(q, axis=1))
                }
                with open(checkpoint_file + '.tmp', 'wb') as f:
                    pickle.dump(quick_checkpoint, f)
                # åŸå­æ€§ä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
                if os.path.exists(checkpoint_file + '.tmp'):
                    os.rename(checkpoint_file + '.tmp', checkpoint_file)
            except Exception as e:
                # é™é»˜å¤„ç†å¿«é€Ÿä¿å­˜å¤±è´¥
                pass

    env.close()
    
    # è®­ç»ƒå®Œæˆåæ¢è¡Œï¼Œè®©è¿›åº¦æ¡æ˜¾ç¤ºå®Œæ•´
    if not log_details and is_training:
        print()  # æ¢è¡Œ

    # è®­ç»ƒå®Œæˆæ€»ç»“
    if log_details and is_training:
        final_success_rate = np.sum(rewards_per_episode) / episodes * 100
        total_steps = step_count
        avg_final_q = np.mean(np.max(q, axis=1))
        
        print("\n" + "=" * QLearningConfig.SEPARATOR_LENGTH)
        print("ğŸ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   â€¢ æ€»å›åˆæ•°: {episodes}")
        print(f"   â€¢ æ€»æ­¥æ•°: {total_steps}")
        print(f"   â€¢ æˆåŠŸç‡: {final_success_rate:.2f}%")
        print(f"   â€¢ å¹³å‡Qå€¼: {avg_final_q:.4f}")
        print(f"   â€¢ æœ€ç»ˆæ¢ç´¢ç‡: {epsilon:.4f}")
        print(f"   â€¢ æœ€ç»ˆå­¦ä¹ ç‡: {learning_rate_a:.4f}")
        
        # æ˜¾ç¤ºå­¦åˆ°çš„æœ€ä¼˜ç­–ç•¥ç¤ºä¾‹
        print(f"\nğŸ§  å­¦åˆ°çš„ç­–ç•¥ç¤ºä¾‹ (å‰16ä¸ªçŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ):")
        for row in range(QLearningConfig.STRATEGY_DISPLAY_ROWS):
            actions_row = []
            for col in range(QLearningConfig.STRATEGY_DISPLAY_COLS):
                state = row * QLearningConfig.MAP_SIZE + col
                best_action = np.argmax(q[state, :])
                actions_row.append(action_names[best_action])
            print(f"   è¡Œ{row}: {' '.join(f'{action:^4}' for action in actions_row)}")
        print("=" * QLearningConfig.SEPARATOR_LENGTH)

    sum_rewards = np.zeros(episodes)
    for t in range(episodes):
        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-QLearningConfig.MOVING_AVERAGE_WINDOW):(t+1)])
    plt.figure(figsize=QLearningConfig.PLOT_FIGURE_SIZE)
    plt.subplot(2, 1, 1)
    plt.plot(sum_rewards)
    plt.title(f'Q-Learning è®­ç»ƒè¿‡ç¨‹ - ç§»åŠ¨å¹³å‡å¥–åŠ± ({QLearningConfig.MOVING_AVERAGE_WINDOW}å›åˆçª—å£)', fontsize=14)
    plt.xlabel('å›åˆæ•°')
    plt.ylabel(f'ç´¯ç§¯æˆåŠŸæ¬¡æ•° (è¿‡å»{QLearningConfig.MOVING_AVERAGE_WINDOW}å›åˆ)')
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ æˆåŠŸç‡å›¾
    plt.subplot(2, 1, 2)
    success_rate_curve = np.zeros(episodes)
    for t in range(episodes):
        success_rate_curve[t] = np.sum(rewards_per_episode[:t+1]) / (t+1) * 100
    plt.plot(success_rate_curve, 'r-', linewidth=2)
    plt.title('ç´¯ç§¯æˆåŠŸç‡å˜åŒ–', fontsize=14)
    plt.xlabel('å›åˆæ•°')
    plt.ylabel('æˆåŠŸç‡ (%)')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ç¡®ä¿å›¾è¡¨å¯ä»¥æ­£ç¡®ä¿å­˜
    try:
        plt.savefig(QLearningConfig.PLOT_FILE, dpi=QLearningConfig.PLOT_DPI, bbox_inches='tight')
        if log_details:
            print(f"ğŸ“ˆ è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ° {QLearningConfig.PLOT_FILE}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
        # å°è¯•ä¿å­˜åˆ°å½“å‰ç›®å½•
        try:
            plt.savefig('./' + QLearningConfig.PLOT_FILE, dpi=QLearningConfig.PLOT_DPI, bbox_inches='tight')
            if log_details:
                print(f"ğŸ“ˆ è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ° ./{QLearningConfig.PLOT_FILE}")
        except Exception as e2:
            print(f"âŒ ä¿å­˜å›¾è¡¨å®Œå…¨å¤±è´¥: {e2}")
    
    plt.close()  # å…³é—­å›¾è¡¨é‡Šæ”¾å†…å­˜

    if is_training:
        with open(QLearningConfig.MODEL_FILE, "wb") as f:
            pickle.dump(q, f)
        if log_details:
            print(f"ğŸ’¾ è®­ç»ƒæ¨¡å‹å·²ä¿å­˜åˆ° {QLearningConfig.MODEL_FILE}")

if __name__ == '__main__':
    # è®­ç»ƒæ¨¡å¼ç¤ºä¾‹ - å¸¦è¯¦ç»†æ—¥å¿—
    print("ğŸš€ å¯åŠ¨ Q-Learning ç®—æ³•æ¼”ç¤º")
    print("ğŸ’¡ æç¤º: render=True æ˜¾ç¤ºå¯è§†åŒ–ç•Œé¢ï¼Œrender=False åŠ å¿«è®­ç»ƒé€Ÿåº¦")
    print("ğŸ’¡ æç¤º: log_details=False å¯ä»¥å…³é—­è¯¦ç»†æ—¥å¿—")
    print()
    
    # ä½ å¯ä»¥è°ƒæ•´è¿™äº›å‚æ•°:
    # episodes: è®­ç»ƒå›åˆæ•°
    # is_training: True=è®­ç»ƒæ¨¡å¼, False=æµ‹è¯•æ¨¡å¼(éœ€è¦å…ˆè®­ç»ƒ)
    # render: True=æ˜¾ç¤ºå›¾å½¢ç•Œé¢, False=æ— å›¾å½¢ç•Œé¢(æ›´å¿«)
    # log_details: True=æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—, False=é™é»˜è¿è¡Œ
    
    # ğŸš€ å¿«é€Ÿè®­ç»ƒé˜¶æ®µ (æ— å¯è§†åŒ–ï¼Œå¿«é€Ÿå­¦ä¹ )
    print(f"ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒ ({QLearningConfig.DEFAULT_TRAINING_EPISODES}å›åˆ)...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„checkpoint
    checkpoint_file = QLearningConfig.CHECKPOINT_FILE
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'rb') as f:
                checkpoint = pickle.load(f)
                print(f"ğŸ” å‘ç°ä¹‹å‰çš„checkpoint: å·²è®­ç»ƒåˆ°ç¬¬{checkpoint['episode']}å›åˆ")
                print(f"   æˆåŠŸç‡: {checkpoint['success_rate']:.1f}%, å¹³å‡Qå€¼: {checkpoint['avg_q']:.4f}")
                
                # è‡ªåŠ¨ç»§ç»­è®­ç»ƒï¼Œæ— éœ€ç”¨æˆ·é€‰æ‹©
                print("âœ… è‡ªåŠ¨ç»§ç»­ä¹‹å‰çš„è®­ç»ƒ...")
                # è®¡ç®—å‰©ä½™å›åˆæ•°
                remaining_episodes = QLearningConfig.DEFAULT_TRAINING_EPISODES - checkpoint['episode']
                if remaining_episodes > 0:
                    print(f"ğŸ“Š å‰©ä½™è®­ç»ƒå›åˆæ•°: {remaining_episodes}")
                    run(episodes=remaining_episodes, is_training=True, render=False, log_details=False, checkpoint_file=checkpoint_file)
                else:
                    print("ğŸ‰ è®­ç»ƒå·²å®Œæˆï¼")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}, é‡æ–°å¼€å§‹è®­ç»ƒ")
            run(episodes=QLearningConfig.DEFAULT_TRAINING_EPISODES, is_training=True, render=False, log_details=False, checkpoint_file=checkpoint_file)
    else:
        print("ğŸ†• å¼€å§‹æ–°è®­ç»ƒ...")
        run(episodes=QLearningConfig.DEFAULT_TRAINING_EPISODES, is_training=True, render=False, log_details=False, checkpoint_file=checkpoint_file)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼ç°åœ¨å¼€å§‹å¯è§†åŒ–æ¼”ç¤º...")
    input("æŒ‰Enteré”®å¼€å§‹æ¼”ç¤º...")
    
    # ğŸ® å¯è§†åŒ–æ¼”ç¤ºé˜¶æ®µ (æ˜¾ç¤ºè®­ç»ƒåçš„æ•ˆæœï¼Œä¸é™å›åˆæ•°)
    print("\n" + "="*QLearningConfig.DEMO_SEPARATOR_LENGTH)
    print("ğŸ® æ¼”ç¤ºè®­ç»ƒåçš„æ™ºèƒ½ä½“è¡¨ç° (ä¸é™å›åˆæ•°ï¼ŒæŒ‰Ctrl+Cåœæ­¢)")
    try:
        run(episodes=QLearningConfig.DEMO_EPISODES, is_training=False, render=True, log_details=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºç»“æŸ")
