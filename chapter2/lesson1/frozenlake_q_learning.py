import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import sys

def run(episodes, is_training=True, render=False, log_details=True):

    env = gym.make('FrozenLake-v1', map_name="8x8", is_slippery=True, render_mode='human' if render else None)

    if(is_training):
        q = np.zeros((env.observation_space.n, env.action_space.n)) # init a 64 x 4 array
    else:
        f = open('frozen_lake8x8.pkl', 'rb')
        q = pickle.load(f)
        f.close()

    learning_rate_a = 0.9 # alpha or learning rate
    discount_factor_g = 0.9 # gamma or discount rate. Near 0: more weight/reward placed on immediate state. Near 1: more on future state.
    epsilon = 1         # 1 = 100% random actions
    epsilon_decay_rate = 0.0001        # epsilon decay rate. 1/0.0001 = 10,000
    rng = np.random.default_rng()   # random number generator

    rewards_per_episode = np.zeros(episodes)
    
    # æ—¥å¿—ç›¸å…³å˜é‡
    action_names = ["å·¦", "ä¸‹", "å³", "ä¸Š"]
    log_interval = max(1, episodes // 100)  # æ¯1%çš„è¿›åº¦è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ—¥å¿—
    step_count = 0
    
    if log_details and is_training:
        print("=" * 80)
        print("ğŸ¯ Q-Learning ç®—æ³•è®­ç»ƒå¼€å§‹")
        print(f"ğŸ“Š ç¯å¢ƒ: FrozenLake 8x8, æ€»å›åˆæ•°: {episodes}")
        print(f"ğŸ§  åˆå§‹å‚æ•°: Î±(å­¦ä¹ ç‡)={learning_rate_a}, Î³(æŠ˜æ‰£å› å­)={discount_factor_g}, Îµ(æ¢ç´¢ç‡)={epsilon}")
        print("=" * 80)

    for i in range(episodes):
        state = env.reset()[0]  # states: 0 to 63, 0=top left corner,63=bottom right corner
        terminated = False      # True when fall in hole or reached goal
        truncated = False       # True when actions > 200
        episode_steps = 0
        episode_reward = 0

        # è®°å½•å›åˆå¼€å§‹ä¿¡æ¯
        show_episode_detail = log_details and is_training and (i % log_interval == 0 or i < 5)
        
        if show_episode_detail:
            print(f"\nğŸ“ å›åˆ {i+1}/{episodes} (è¿›åº¦: {(i+1)/episodes*100:.1f}%)")
            print(f"ğŸ² å½“å‰æ¢ç´¢ç‡ Îµ = {epsilon:.4f}, å­¦ä¹ ç‡ Î± = {learning_rate_a:.4f}")
            print(f"ğŸ—ºï¸  èµ·å§‹ä½ç½®: çŠ¶æ€ {state} (ä½ç½®: è¡Œ{state//8}, åˆ—{state%8})")

        while(not terminated and not truncated):
            step_count += 1
            episode_steps += 1
            
            # è¯¦ç»†æ—¥å¿— - æ­¥éª¤å¼€å§‹
            if show_episode_detail and episode_steps <= 15:  # å¢åŠ åˆ°15æ­¥
                print(f"\n  ğŸ“ æ­¥éª¤ {episode_steps}: å½“å‰çŠ¶æ€ {state} (è¡Œ{state//8}, åˆ—{state%8})")
                print(f"     å½“å‰çŠ¶æ€Qå€¼: {q[state,:]}")
                sys.stdout.flush()
            
            # åŠ¨ä½œé€‰æ‹©ç­–ç•¥
            random_value = rng.random() if is_training else 0
            if is_training and random_value < epsilon:
                action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up
                action_type = "ğŸ² éšæœºæ¢ç´¢"
                if show_episode_detail and episode_steps <= 15:
                    print(f"     å†³ç­–ä¾æ®: éšæœºæ•° {random_value:.4f} < Îµ({epsilon:.4f}) â†’ éšæœºé€‰æ‹©")
                    sys.stdout.flush()
            else:
                action = np.argmax(q[state,:])
                action_type = "ğŸ§  ç­–ç•¥é€‰æ‹©"
                if show_episode_detail and episode_steps <= 15:
                    best_q_values = q[state,:]
                    print(f"     å†³ç­–ä¾æ®: é€‰æ‹©æœ€å¤§Qå€¼åŠ¨ä½œ")
                    for act_idx, (act_name, q_val) in enumerate(zip(action_names, best_q_values)):
                        marker = " âœ…" if act_idx == action else ""
                        print(f"       åŠ¨ä½œ'{act_name}': Q={q_val:.4f}{marker}")
                    sys.stdout.flush()
                
            # è®°å½•åŠ¨ä½œå‰çš„Qå€¼
            old_q_value = q[state, action] if is_training else 0
            
            if show_episode_detail and episode_steps <= 15:
                print(f"     â¡ï¸  é€‰æ‹©åŠ¨ä½œ: '{action_names[action]}' ({action_type})")
                sys.stdout.flush()

            new_state,reward,terminated,truncated,_ = env.step(action)
            episode_reward += reward

            if is_training:
                # ğŸ¯ å¥–åŠ±å¡‘é€ : ä¸ºå†°æ´æ·»åŠ è´Ÿå¥–åŠ±æ¥æ”¹å–„å­¦ä¹ 
                if terminated and reward == 0.0:  # æ‰å…¥å†°æ´
                    shaped_reward = -0.1  # è´Ÿå¥–åŠ±æƒ©ç½šæ‰å…¥å†°æ´
                else:
                    shaped_reward = reward  # ä¿æŒåŸå§‹å¥–åŠ±
                
                # Q-learning æ›´æ–°å…¬å¼è¯¦ç»†è¿‡ç¨‹
                max_next_q = np.max(q[new_state,:])
                target = shaped_reward + discount_factor_g * max_next_q
                td_error = target - old_q_value
                new_q_value = old_q_value + learning_rate_a * td_error
                q[state,action] = new_q_value
                
                # è¯¦ç»†æ—¥å¿—è¾“å‡º - Qå€¼æ›´æ–°è¿‡ç¨‹
                if show_episode_detail and episode_steps <= 15:
                    print(f"     ğŸ”„ çŠ¶æ€è½¬ç§»: {state} â†’ {new_state} (è¡Œ{new_state//8}, åˆ—{new_state%8})")
                    print(f"     ğŸ† ç¯å¢ƒå¥–åŠ±: r = {reward}")
                    if shaped_reward != reward:
                        print(f"     ğŸ¯ å¥–åŠ±å¡‘é€ : shaped_r = {shaped_reward} (æ‰å…¥å†°æ´æƒ©ç½š -0.1)")
                    else:
                        print(f"     ğŸ¯ æœ€ç»ˆå¥–åŠ±: shaped_r = {shaped_reward}")
                    print(f"     ğŸ“Š Q-Learning æ›´æ–°è®¡ç®—è¯¦ç»†è¿‡ç¨‹:")
                    print(f"       ğŸ”¹ æ­¥éª¤1: è·å–å½“å‰Qå€¼")
                    print(f"         Q(s={state}, a={action}) = {old_q_value:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤2: æŸ¥çœ‹æ–°çŠ¶æ€çš„æ‰€æœ‰Qå€¼")
                    print(f"         Q({new_state}, :) = {q[new_state,:]}")
                    print(f"       ğŸ”¹ æ­¥éª¤3: æ‰¾åˆ°æ–°çŠ¶æ€çš„æœ€å¤§Qå€¼")
                    print(f"         max Q(s'={new_state}, a') = {max_next_q:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤4: ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—ç›®æ ‡å€¼")
                    print(f"         Target = shaped_r + Î³ Ã— max Q(s', a')")
                    print(f"         Target = {shaped_reward} + {discount_factor_g} Ã— {max_next_q:.4f}")
                    print(f"         Target = {target:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤5: è®¡ç®—æ—¶åºå·®åˆ†(TD)è¯¯å·®")
                    print(f"         TD_error = Target - Q(s,a)")
                    print(f"         TD_error = {target:.4f} - {old_q_value:.4f}")
                    print(f"         TD_error = {td_error:.4f}")
                    print(f"       ğŸ”¹ æ­¥éª¤6: ä½¿ç”¨å­¦ä¹ ç‡æ›´æ–°Qå€¼")
                    print(f"         Q_new(s,a) = Q_old(s,a) + Î± Ã— TD_error")
                    print(f"         Q_new({state},{action}) = {old_q_value:.4f} + {learning_rate_a} Ã— {td_error:.4f}")
                    print(f"         Q_new({state},{action}) = {new_q_value:.4f}")
                    print(f"     ğŸ“‹ æ›´æ–°åçš„Qè¡¨çŠ¶æ€:")
                    print(f"       å½“å‰çŠ¶æ€Qå€¼: Q({state},:) = {q[state,:]}")
                    
                    if terminated:
                        if reward > 0:
                            print(f"     ğŸ‰ åˆ°è¾¾ç›®æ ‡! è·å¾—å¥–åŠ± {reward}")
                        else:
                            print(f"     â„ï¸ æ‰å…¥å†°æ´! å›åˆç»“æŸ")
                    
                    print(f"     {'='*70}")
                    # æ¯ä¸ªåŠ¨ä½œæ­¥éª¤åç«‹å³åˆ·æ–°è¾“å‡º
                    sys.stdout.flush()
            else:
                if show_episode_detail and episode_steps <= 15:
                    print(f"     ğŸ”„ çŠ¶æ€è½¬ç§»: {state} â†’ {new_state} (æµ‹è¯•æ¨¡å¼ï¼Œä¸æ›´æ–°Qå€¼)")
                    sys.stdout.flush()

            state = new_state

        epsilon = max(epsilon - epsilon_decay_rate, 0)

        if(epsilon==0):
            learning_rate_a = 0.0001

        if reward == 1:
            rewards_per_episode[i] = 1
            
        # å›åˆæ€»ç»“
        if show_episode_detail:
            success_rate = np.sum(rewards_per_episode[:i+1]) / (i+1) * 100
            print(f"  ğŸ“‹ å›åˆæ€»ç»“: {episode_steps}æ­¥, å¥–åŠ±={episode_reward}, å½“å‰æˆåŠŸç‡={success_rate:.1f}%")
            
        # è¿›åº¦æ›´æ–°
        if log_details and is_training and i % (episodes // 20) == 0 and i > 0:
            success_rate = np.sum(rewards_per_episode[:i+1]) / (i+1) * 100
            avg_q = np.mean(np.max(q, axis=1))
            print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦ {i+1}/{episodes} ({(i+1)/episodes*100:.0f}%): "
                  f"æˆåŠŸç‡={success_rate:.1f}%, å¹³å‡Qå€¼={avg_q:.3f}, Îµ={epsilon:.4f}")
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

    env.close()

    # è®­ç»ƒå®Œæˆæ€»ç»“
    if log_details and is_training:
        final_success_rate = np.sum(rewards_per_episode) / episodes * 100
        total_steps = step_count
        avg_final_q = np.mean(np.max(q, axis=1))
        
        print("\n" + "=" * 80)
        print("ğŸ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   â€¢ æ€»å›åˆæ•°: {episodes}")
        print(f"   â€¢ æ€»æ­¥æ•°: {total_steps}")
        print(f"   â€¢ æˆåŠŸç‡: {final_success_rate:.2f}%")
        print(f"   â€¢ å¹³å‡Qå€¼: {avg_final_q:.4f}")
        print(f"   â€¢ æœ€ç»ˆæ¢ç´¢ç‡: {epsilon:.4f}")
        print(f"   â€¢ æœ€ç»ˆå­¦ä¹ ç‡: {learning_rate_a:.4f}")
        
        # æ˜¾ç¤ºå­¦åˆ°çš„æœ€ä¼˜ç­–ç•¥ç¤ºä¾‹
        print(f"\nğŸ§  å­¦åˆ°çš„ç­–ç•¥ç¤ºä¾‹ (å‰16ä¸ªçŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ):")
        for row in range(4):
            actions_row = []
            for col in range(4):
                state = row * 8 + col
                best_action = np.argmax(q[state, :])
                actions_row.append(action_names[best_action])
            print(f"   è¡Œ{row}: {' '.join(f'{action:^4}' for action in actions_row)}")
        print("=" * 80)

    sum_rewards = np.zeros(episodes)
    for t in range(episodes):
        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])
    plt.figure(figsize=(12, 8))
    plt.subplot(2, 1, 1)
    plt.plot(sum_rewards)
    plt.title('Q-Learning è®­ç»ƒè¿‡ç¨‹ - ç§»åŠ¨å¹³å‡å¥–åŠ± (100å›åˆçª—å£)', fontsize=14)
    plt.xlabel('å›åˆæ•°')
    plt.ylabel('ç´¯ç§¯æˆåŠŸæ¬¡æ•° (è¿‡å»100å›åˆ)')
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ æˆåŠŸç‡å›¾
    plt.subplot(2, 1, 2)
    success_rate_curve = np.zeros(episodes)
    for t in range(episodes):
        success_rate_curve[t] = np.sum(rewards_per_episode[:t+1]) / (t+1) * 100
    plt.plot(success_rate_curve, 'r-', linewidth=2)
    plt.title('ç´¯ç§¯æˆåŠŸç‡å˜åŒ–', fontsize=14)
    plt.xlabel('å›åˆæ•°')
    plt.ylabel('æˆåŠŸç‡ (%)')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('frozen_lake8x8.png', dpi=150, bbox_inches='tight')
    if log_details:
        print(f"ğŸ“ˆ è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ° frozen_lake8x8.png")

    if is_training:
        f = open("frozen_lake8x8.pkl","wb")
        pickle.dump(q, f)
        f.close()
        if log_details:
            print(f"ğŸ’¾ è®­ç»ƒæ¨¡å‹å·²ä¿å­˜åˆ° frozen_lake8x8.pkl")

if __name__ == '__main__':
    # è®­ç»ƒæ¨¡å¼ç¤ºä¾‹ - å¸¦è¯¦ç»†æ—¥å¿—
    print("ğŸš€ å¯åŠ¨ Q-Learning ç®—æ³•æ¼”ç¤º")
    print("ğŸ’¡ æç¤º: render=True æ˜¾ç¤ºå¯è§†åŒ–ç•Œé¢ï¼Œrender=False åŠ å¿«è®­ç»ƒé€Ÿåº¦")
    print("ğŸ’¡ æç¤º: log_details=False å¯ä»¥å…³é—­è¯¦ç»†æ—¥å¿—")
    print()
    
    # ä½ å¯ä»¥è°ƒæ•´è¿™äº›å‚æ•°:
    # episodes: è®­ç»ƒå›åˆæ•°
    # is_training: True=è®­ç»ƒæ¨¡å¼, False=æµ‹è¯•æ¨¡å¼(éœ€è¦å…ˆè®­ç»ƒ)
    # render: True=æ˜¾ç¤ºå›¾å½¢ç•Œé¢, False=æ— å›¾å½¢ç•Œé¢(æ›´å¿«)
    # log_details: True=æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—, False=é™é»˜è¿è¡Œ
    
    # ğŸ® æ–¹æ¡ˆ1: å¸¦å¯è§†åŒ–ç•Œé¢çš„è®­ç»ƒ (é€Ÿåº¦è¾ƒæ…¢ä½†èƒ½çœ‹åˆ°agentç§»åŠ¨)
    run(episodes=100, is_training=True, render=True, log_details=True)
    
    # ğŸš€ æ–¹æ¡ˆ2: å¿«é€Ÿè®­ç»ƒ (å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œï¼Œæ³¨é‡Šä¸Šé¢é‚£è¡Œ)
    # run(episodes=1000, is_training=True, render=False, log_details=True)
    
    # è®­ç»ƒå®Œæˆåçš„æµ‹è¯•è¿è¡Œ (å¯é€‰)
    print("\n" + "="*50)
    print("ğŸ® æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ (è¿è¡Œ3å›åˆ)")
    run(episodes=3, is_training=False, render=True, log_details=True)
