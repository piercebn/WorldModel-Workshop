import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import sys

def discretize_state(state, bins):
    """å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–ä¸ºç¦»æ•£çŠ¶æ€"""
    cart_pos, cart_vel, pole_angle, pole_vel = state
    
    # å®šä¹‰çŠ¶æ€ç©ºé—´çš„è¾¹ç•Œ
    cart_pos_bins = np.linspace(-2.4, 2.4, bins)
    cart_vel_bins = np.linspace(-3.0, 3.0, bins)
    pole_angle_bins = np.linspace(-0.5, 0.5, bins)
    pole_vel_bins = np.linspace(-2.0, 2.0, bins)
    
    # ç¦»æ•£åŒ–å„ä¸ªçŠ¶æ€ç»´åº¦
    cart_pos_idx = np.digitize(cart_pos, cart_pos_bins) - 1
    cart_vel_idx = np.digitize(cart_vel, cart_vel_bins) - 1
    pole_angle_idx = np.digitize(pole_angle, pole_angle_bins) - 1
    pole_vel_idx = np.digitize(pole_vel, pole_vel_bins) - 1
    
    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
    cart_pos_idx = np.clip(cart_pos_idx, 0, bins-1)
    cart_vel_idx = np.clip(cart_vel_idx, 0, bins-1)
    pole_angle_idx = np.clip(pole_angle_idx, 0, bins-1)
    pole_vel_idx = np.clip(pole_vel_idx, 0, bins-1)
    
    return cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_vel_idx

def run(is_training=True, render=True, log_details=True):
    env = gym.make('CartPole-v1', render_mode='human' if render else None)
    
    bins = 10  # æ¯ä¸ªçŠ¶æ€ç»´åº¦çš„ç¦»æ•£åŒ–æ•°é‡
    
    if is_training:
        q = np.zeros((bins, bins, bins, bins, env.action_space.n))
        print("ğŸš€ å¼€å§‹CartPole Q-Learningè®­ç»ƒ")
        print("="*50)
    else:
        try:
            with open('cartpole.pkl', 'rb') as f:
                q = pickle.load(f)
            print("ğŸ¯ åŠ è½½å·²è®­ç»ƒçš„Qè¡¨ï¼Œå¼€å§‹æµ‹è¯•")
            print("="*50)
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ cartpole.pkl")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼")
            return
    
    learning_rate = 0.1
    discount_factor = 0.99
    epsilon = 1.0 if is_training else 0.0
    epsilon_decay = 0.995
    min_epsilon = 0.01
    
    episodes = 1000 if is_training else 5
    rewards_per_episode = []
    
    for episode in range(episodes):
        state, _ = env.reset()
        state_discrete = discretize_state(state, bins)
        total_reward = 0
        step = 0
        
        if log_details:
            if is_training:
                print(f"\nğŸ“ è®­ç»ƒå›åˆ {episode+1}/{episodes}")
                print(f"ğŸ² æ¢ç´¢ç‡: {epsilon:.3f}")
            else:
                print(f"\nğŸ¯ æµ‹è¯•å›åˆ {episode+1}/{episodes}")
            print(f"ğŸ åˆå§‹çŠ¶æ€: {state}")
            print(f"ğŸ”¢ ç¦»æ•£çŠ¶æ€: {state_discrete}")
            sys.stdout.flush()
        
        while True:
            step += 1
            
            # é€‰æ‹©åŠ¨ä½œ
            if is_training and np.random.random() < epsilon:
                action = env.action_space.sample()
                if log_details:
                    print(f"ğŸ² æ­¥éª¤ {step}: éšæœºæ¢ç´¢é€‰æ‹©åŠ¨ä½œ {action} ({'å‘å·¦æ¨è½¦' if action == 0 else 'å‘å³æ¨è½¦'})")
            else:
                q_values = q[state_discrete]
                action = np.argmax(q_values)
                if log_details:
                    print(f"ğŸ§  æ­¥éª¤ {step}: ç­–ç•¥é€‰æ‹©åŠ¨ä½œ {action} ({'å‘å·¦æ¨è½¦' if action == 0 else 'å‘å³æ¨è½¦'})")
                    print(f"   Qå€¼: [å‘å·¦={q_values[0]:.3f}, å‘å³={q_values[1]:.3f}]")
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            if render:
                try:
                    env.render()
                except:
                    pass
            
            next_state_discrete = discretize_state(next_state, bins)
            total_reward += reward
            
            if log_details:
                print(f"   âš¡ å¥–åŠ±: {reward}, ç´¯è®¡å¥–åŠ±: {total_reward}")
                print(f"   ğŸ”„ æ–°çŠ¶æ€: {next_state}")
                print(f"   ğŸ”¢ æ–°ç¦»æ•£çŠ¶æ€: {next_state_discrete}")
                sys.stdout.flush()
            
            # Q-Learningæ›´æ–°
            if is_training:
                old_q_value = q[state_discrete][action]
                max_next_q = np.max(q[next_state_discrete])
                
                # è´å°”æ›¼æ–¹ç¨‹æ›´æ–°
                target_q = reward + discount_factor * max_next_q
                td_error = target_q - old_q_value
                new_q_value = old_q_value + learning_rate * td_error
                
                q[state_discrete][action] = new_q_value
                
                if log_details:
                    print(f"   ğŸ“Š Qå€¼æ›´æ–°:")
                    print(f"      æ—§Qå€¼: {old_q_value:.3f}")
                    print(f"      æœ€å¤§ä¸‹ä¸€Qå€¼: {max_next_q:.3f}")
                    print(f"      ç›®æ ‡Qå€¼: {target_q:.3f}")
                    print(f"      TDè¯¯å·®: {td_error:.3f}")
                    print(f"      æ–°Qå€¼: {new_q_value:.3f}")
                    sys.stdout.flush()
            
            state = next_state
            state_discrete = next_state_discrete
            
            if terminated or truncated:
                break
        
        rewards_per_episode.append(total_reward)
        
        if log_details:
            print(f"âœ… å›åˆç»“æŸ - æ€»æ­¥æ•°: {step}, æ€»å¥–åŠ±: {total_reward}")
            if episode % 100 == 0 and is_training:
                avg_reward = np.mean(rewards_per_episode[-100:])
                print(f"ğŸ“ˆ æœ€è¿‘100å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
            sys.stdout.flush()
        
        # æ›´æ–°epsilon
        if is_training:
            epsilon = max(min_epsilon, epsilon * epsilon_decay)
    
    env.close()
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    if is_training:
        with open('cartpole.pkl', 'wb') as f:
            pickle.dump(q, f)
        print(f"\nğŸ’¾ Qè¡¨å·²ä¿å­˜åˆ° cartpole.pkl")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(rewards_per_episode)
        plt.title('Training Rewards per Episode')
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.grid(True)
        
        # ç§»åŠ¨å¹³å‡
        window_size = 100
        if len(rewards_per_episode) >= window_size:
            moving_avg = []
            for i in range(len(rewards_per_episode) - window_size + 1):
                moving_avg.append(np.mean(rewards_per_episode[i:i+window_size]))
            
            plt.subplot(1, 2, 2)
            plt.plot(moving_avg)
            plt.title(f'Moving Average Reward (window={window_size})')
            plt.xlabel('Episode')
            plt.ylabel('Average Reward')
            plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('cartpole_training.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° cartpole_training.png")
    
    print(f"\nğŸ‰ {'è®­ç»ƒ' if is_training else 'æµ‹è¯•'}å®Œæˆ!")
    print(f"ğŸ“Š å¹³å‡å¥–åŠ±: {np.mean(rewards_per_episode):.2f}")

if __name__ == '__main__':
    print("ğŸ¤– CartPole Q-Learning æ¼”ç¤º")
    print("ğŸ’¡ Q-Learning: é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•")
    print()
    
    # æä¾›ä¸åŒçš„è¿è¡Œæ¨¡å¼
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿè®­ç»ƒæ¨¡å¼ (æ— å¯è§†åŒ–)")
    print("2. å¯è§†åŒ–è®­ç»ƒæ¨¡å¼ (æœ‰å¯è§†åŒ–)")
    print("3. ä»…æµ‹è¯•æ¨¡å¼ (éœ€è¦å…ˆè®­ç»ƒ)")
    
    try:
        mode = int(input("è¯·è¾“å…¥æ¨¡å¼ç¼–å· (1-3): "))
    except:
        mode = 2  # é»˜è®¤å¯è§†åŒ–è®­ç»ƒ
    
    if mode == 1:
        print("ğŸš€ å¿«é€Ÿè®­ç»ƒæ¨¡å¼")
        run(is_training=True, render=False, log_details=False)
    elif mode == 2:
        print("ğŸ® å¯è§†åŒ–è®­ç»ƒæ¨¡å¼")
        run(is_training=True, render=True, log_details=True)
    elif mode == 3:
        print("ğŸ¯ æµ‹è¯•æ¨¡å¼")
        run(is_training=False, render=True, log_details=True)
    else:
        print("âŒ æ— æ•ˆæ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤å¯è§†åŒ–è®­ç»ƒæ¨¡å¼")
        run(is_training=True, render=True, log_details=True)