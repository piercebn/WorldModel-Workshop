import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import sys
import os

def discretize_state(state, bins):
    """å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–ä¸ºç¦»æ•£çŠ¶æ€"""
    cart_pos, cart_vel, pole_angle, pole_vel = state
    
    # å®šä¹‰çŠ¶æ€ç©ºé—´çš„è¾¹ç•Œ
    cart_pos_bins = np.linspace(-2.4, 2.4, bins)
    cart_vel_bins = np.linspace(-3.0, 3.0, bins)
    pole_angle_bins = np.linspace(-0.5, 0.5, bins)
    pole_vel_bins = np.linspace(-2.0, 2.0, bins)
    
    # ç¦»æ•£åŒ–å„ä¸ªçŠ¶æ€ç»´åº¦
    cart_pos_idx = np.digitize(cart_pos, cart_pos_bins) - 1
    cart_vel_idx = np.digitize(cart_vel, cart_vel_bins) - 1
    pole_angle_idx = np.digitize(pole_angle, pole_angle_bins) - 1
    pole_vel_idx = np.digitize(pole_vel, pole_vel_bins) - 1
    
    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
    cart_pos_idx = np.clip(cart_pos_idx, 0, bins-1)
    cart_vel_idx = np.clip(cart_vel_idx, 0, bins-1)
    pole_angle_idx = np.clip(pole_angle_idx, 0, bins-1)
    pole_vel_idx = np.clip(pole_vel_idx, 0, bins-1)
    
    return cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_vel_idx

def run(is_training=True, render=True, log_details=True, checkpoint_file='cartpole_checkpoint.pkl'):
    env = gym.make('CartPole-v1', render_mode='human' if render else None)
    
    bins = 10  # æ¯ä¸ªçŠ¶æ€ç»´åº¦çš„ç¦»æ•£åŒ–æ•°é‡
    
    # å°è¯•åŠ è½½ä¹‹å‰çš„checkpoint
    if is_training and os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'rb') as f:
                checkpoint = pickle.load(f)
                q = checkpoint['q_table']
                start_episode = checkpoint['episode']
                epsilon = checkpoint['epsilon']
                learning_rate = checkpoint['learning_rate']
                print(f"âœ… åŠ è½½checkpoint: ä»ç¬¬{start_episode}å›åˆç»§ç»­è®­ç»ƒ")
                print(f"   å½“å‰Qè¡¨çŠ¶æ€: å¹³å‡Qå€¼={np.mean(np.max(q, axis=-1)):.4f}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}, ä»å¤´å¼€å§‹è®­ç»ƒ")
            q = np.zeros((bins, bins, bins, bins, env.action_space.n))
            start_episode = 0
            epsilon = 1.0
            learning_rate = 0.1
    elif is_training:
        q = np.zeros((bins, bins, bins, bins, env.action_space.n))
        start_episode = 0
        epsilon = 1.0
        learning_rate = 0.1
        print("ğŸš€ å¼€å§‹CartPole Q-Learningè®­ç»ƒ")
        print("="*50)
    else:
        # æµ‹è¯•æ¨¡å¼ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        try:
            with open('cartpole.pkl', 'rb') as f:
                q = pickle.load(f)
            print("ğŸ¯ åŠ è½½å·²è®­ç»ƒçš„Qè¡¨ï¼Œå¼€å§‹æµ‹è¯•")
            print("="*50)
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ 'cartpole.pkl'")
            print("è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼")
            return
    
    discount_factor = 0.99
    epsilon_decay = 0.995
    min_epsilon = 0.01
    
    episodes = 50000 if is_training else 5
    rewards_per_episode = []
    
    for episode in range(episodes):
        # æ˜¾ç¤ºè¿›åº¦æ¡
        if not log_details and is_training:
            # è®¡ç®—æ€»ä½“è¿›åº¦ï¼ˆåŒ…æ‹¬ä¹‹å‰è®­ç»ƒçš„å›åˆæ•°ï¼‰
            total_episodes_trained = start_episode + episode + 1
            total_target_episodes = 50000  # æ€»ç›®æ ‡å›åˆæ•°
            progress = total_episodes_trained / total_target_episodes * 100
            bar_length = 30
            filled_length = int(bar_length * progress / 100)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            print(f'\rğŸš€ è®­ç»ƒè¿›åº¦: [{bar}] {progress:.1f}% ({total_episodes_trained}/{total_target_episodes})', end='', flush=True)
        
        state, _ = env.reset()
        state_discrete = discretize_state(state, bins)
        total_reward = 0
        step = 0
        
        if log_details:
            if is_training:
                print(f"\nğŸ“ è®­ç»ƒå›åˆ {episode+1}/{episodes}")
                print(f"ğŸ² æ¢ç´¢ç‡: {epsilon:.3f}")
            else:
                print(f"\nğŸ¯ æµ‹è¯•å›åˆ {episode+1}/{episodes}")
            print(f"ğŸ åˆå§‹çŠ¶æ€: {state}")
            print(f"ğŸ”¢ ç¦»æ•£çŠ¶æ€: {state_discrete}")
            sys.stdout.flush()
        
        while True:
            step += 1
            
            # é€‰æ‹©åŠ¨ä½œ
            if is_training and np.random.random() < epsilon:
                action = env.action_space.sample()
                if log_details:
                    print(f"ğŸ² æ­¥éª¤ {step}: éšæœºæ¢ç´¢é€‰æ‹©åŠ¨ä½œ {action} ({'å‘å·¦æ¨è½¦' if action == 0 else 'å‘å³æ¨è½¦'})")
            else:
                q_values = q[state_discrete]
                action = np.argmax(q_values)
                if log_details:
                    print(f"ğŸ§  æ­¥éª¤ {step}: ç­–ç•¥é€‰æ‹©åŠ¨ä½œ {action} ({'å‘å·¦æ¨è½¦' if action == 0 else 'å‘å³æ¨è½¦'})")
                    print(f"   Qå€¼: [å‘å·¦={q_values[0]:.3f}, å‘å³={q_values[1]:.3f}]")
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            if render:
                try:
                    env.render()
                except:
                    pass
            
            next_state_discrete = discretize_state(next_state, bins)
            total_reward += reward
            
            if log_details:
                print(f"   âš¡ å¥–åŠ±: {reward}, ç´¯è®¡å¥–åŠ±: {total_reward}")
                print(f"   ğŸ”„ æ–°çŠ¶æ€: {next_state}")
                print(f"   ğŸ”¢ æ–°ç¦»æ•£çŠ¶æ€: {next_state_discrete}")
                sys.stdout.flush()
            
            # Q-Learningæ›´æ–°
            if is_training:
                old_q_value = q[state_discrete][action]
                max_next_q = np.max(q[next_state_discrete])
                
                # è´å°”æ›¼æ–¹ç¨‹æ›´æ–°
                target_q = reward + discount_factor * max_next_q
                td_error = target_q - old_q_value
                new_q_value = old_q_value + learning_rate * td_error
                
                q[state_discrete][action] = new_q_value
                
                if log_details:
                    print(f"   ğŸ“Š Qå€¼æ›´æ–°:")
                    print(f"      æ—§Qå€¼: {old_q_value:.3f}")
                    print(f"      æœ€å¤§ä¸‹ä¸€Qå€¼: {max_next_q:.3f}")
                    print(f"      ç›®æ ‡Qå€¼: {target_q:.3f}")
                    print(f"      TDè¯¯å·®: {td_error:.3f}")
                    print(f"      æ–°Qå€¼: {new_q_value:.3f}")
                    sys.stdout.flush()
            
            state = next_state
            state_discrete = next_state_discrete
            
            if terminated or truncated:
                break
        
        rewards_per_episode.append(total_reward)
        
        if log_details:
            print(f"âœ… å›åˆç»“æŸ - æ€»æ­¥æ•°: {step}, æ€»å¥–åŠ±: {total_reward}")
            if episode % 100 == 0 and is_training:
                avg_reward = np.mean(rewards_per_episode[-100:])
                print(f"ğŸ“ˆ æœ€è¿‘100å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                
                # æ¯1000å›åˆä¿å­˜ä¸€æ¬¡checkpoint
                if episode % 1000 == 0 and episode > 0:
                    checkpoint = {
                        'q_table': q,
                        'episode': episode + 1,
                        'epsilon': epsilon,
                        'learning_rate': learning_rate,
                        'avg_reward': avg_reward
                    }
                    with open(checkpoint_file, 'wb') as f:
                        pickle.dump(checkpoint, f)
                    print(f"ğŸ’¾ å·²ä¿å­˜checkpointåˆ° {checkpoint_file}")
                
                # æ¯100å›åˆå¿«é€Ÿä¿å­˜ï¼ˆé˜²æ­¢é¢‘ç¹ä¸­æ–­ï¼‰
                if episode % 100 == 0 and episode > 0:
                    quick_checkpoint = {
                        'q_table': q,
                        'episode': episode + 1,
                        'epsilon': epsilon,
                        'learning_rate': learning_rate,
                        'avg_reward': avg_reward
                    }
                    with open(checkpoint_file + '.tmp', 'wb') as f:
                        pickle.dump(quick_checkpoint, f)
                    # åŸå­æ€§ä¿å­˜ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
                    if os.path.exists(checkpoint_file + '.tmp'):
                        os.rename(checkpoint_file + '.tmp', checkpoint_file)
            
            sys.stdout.flush()
        
        # æ›´æ–°epsilon
        if is_training:
            epsilon = max(min_epsilon, epsilon * epsilon_decay)
    
    env.close()
    
    # è®­ç»ƒå®Œæˆåæ¢è¡Œï¼Œè®©è¿›åº¦æ¡æ˜¾ç¤ºå®Œæ•´
    if not log_details and is_training:
        print()  # æ¢è¡Œ
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    if is_training:
        with open('cartpole.pkl', 'wb') as f:
            pickle.dump(q, f)
        print(f"\nğŸ’¾ Qè¡¨å·²ä¿å­˜åˆ° cartpole.pkl")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(rewards_per_episode)
        plt.title('Training Rewards per Episode')
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.grid(True)
        
        # ç§»åŠ¨å¹³å‡
        window_size = 100
        if len(rewards_per_episode) >= window_size:
            moving_avg = []
            for i in range(len(rewards_per_episode) - window_size + 1):
                moving_avg.append(np.mean(rewards_per_episode[i:i+window_size]))
            
            plt.subplot(1, 2, 2)
            plt.plot(moving_avg)
            plt.title(f'Moving Average Reward (window={window_size})')
            plt.xlabel('Episode')
            plt.ylabel('Average Reward')
            plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('cartpole_training.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° cartpole_training.png")
    
    print(f"\nğŸ‰ {'è®­ç»ƒ' if is_training else 'æµ‹è¯•'}å®Œæˆ!")
    print(f"ğŸ“Š å¹³å‡å¥–åŠ±: {np.mean(rewards_per_episode):.2f}")

if __name__ == '__main__':
    print("ğŸ¤– CartPole Q-Learning æ¼”ç¤º")
    print("ğŸ’¡ Q-Learning: é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•")
    print()
    
    # æä¾›ä¸åŒçš„è¿è¡Œæ¨¡å¼
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿè®­ç»ƒæ¨¡å¼ (æ— å¯è§†åŒ–)")
    print("2. å¯è§†åŒ–è®­ç»ƒæ¨¡å¼ (æœ‰å¯è§†åŒ–)")
    print("3. ä»…æµ‹è¯•æ¨¡å¼ (éœ€è¦å…ˆè®­ç»ƒ)")
    
    # ğŸš€ å¿«é€Ÿè®­ç»ƒé˜¶æ®µ (æ— å¯è§†åŒ–ï¼Œå¿«é€Ÿå­¦ä¹ )
    print("ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒ (50000å›åˆ)...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„checkpoint
    checkpoint_file = 'cartpole_checkpoint.pkl'
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'rb') as f:
                checkpoint = pickle.load(f)
                print(f"ğŸ” å‘ç°ä¹‹å‰çš„checkpoint: å·²è®­ç»ƒåˆ°ç¬¬{checkpoint['episode']}å›åˆ")
                print(f"   å¹³å‡å¥–åŠ±: {checkpoint['avg_reward']:.1f}")
                
                # è‡ªåŠ¨ç»§ç»­è®­ç»ƒï¼Œæ— éœ€ç”¨æˆ·é€‰æ‹©
                print("âœ… è‡ªåŠ¨ç»§ç»­ä¹‹å‰çš„è®­ç»ƒ...")
                # è®¡ç®—å‰©ä½™å›åˆæ•°
                remaining_episodes = 50000 - checkpoint['episode']
                if remaining_episodes > 0:
                    print(f"ğŸ“Š å‰©ä½™è®­ç»ƒå›åˆæ•°: {remaining_episodes}")
                    run(is_training=True, render=False, log_details=False, checkpoint_file=checkpoint_file)
                else:
                    print("ğŸ‰ è®­ç»ƒå·²å®Œæˆï¼")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}, é‡æ–°å¼€å§‹è®­ç»ƒ")
            run(is_training=True, render=False, log_details=False, checkpoint_file=checkpoint_file)
    else:
        print("ğŸ†• å¼€å§‹æ–°è®­ç»ƒ...")
        run(is_training=True, render=False, log_details=False, checkpoint_file=checkpoint_file)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼ç°åœ¨å¼€å§‹å¯è§†åŒ–æ¼”ç¤º...")
    input("æŒ‰Enteré”®å¼€å§‹æ¼”ç¤º...")
    
    # ğŸ® å¯è§†åŒ–æ¼”ç¤ºé˜¶æ®µ (æ˜¾ç¤ºè®­ç»ƒåçš„æ•ˆæœï¼Œä¸é™å›åˆæ•°)
    print("ğŸ® æ¼”ç¤ºè®­ç»ƒåçš„æ™ºèƒ½ä½“è¡¨ç° (ä¸é™å›åˆæ•°ï¼ŒæŒ‰Ctrl+Cåœæ­¢)")
    try:
        run(is_training=False, render=True, log_details=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºç»“æŸ")