"""
æœ´ç´ Q-Learningç®—æ³•å®ç° - ä¸“æ³¨äºå±•ç¤ºåŸç†
ä½¿ç”¨æœ€ä½³å®è·µçš„ä»£ç ç»“æ„ï¼Œç®€æ´æ˜äº†åœ°å±•ç¤ºQ-Learningçš„æ ¸å¿ƒæ€æƒ³
"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import random
from typing import Tuple, Optional
import os

class QLearningAgent:
    """æœ´ç´ çš„Q-Learningæ™ºèƒ½ä½“"""
    
    def __init__(self, 
                 state_size: int, 
                 action_size: int,
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.95,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 seed: int = 42):
        """
        åˆå§‹åŒ–Q-Learningæ™ºèƒ½ä½“
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´å¤§å°
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            learning_rate: å­¦ä¹ ç‡ (alpha)
            discount_factor: æŠ˜æ‰£å› å­ (gamma)
            epsilon: åˆå§‹æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡ç‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            seed: éšæœºç§å­
        """
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # åˆå§‹åŒ–Qè¡¨
        np.random.seed(seed)
        self.q_table = np.zeros((state_size, action_size))
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_rates = []
        
    def select_action(self, state: int, training: bool = True) -> int:
        """
        ä½¿ç”¨epsilon-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
            
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return np.random.choice(self.action_size)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            return np.argmax(self.q_table[state])
    
    def update_q_table(self, state: int, action: int, reward: float, 
                      next_state: int, done: bool):
        """
        æ›´æ–°Qè¡¨
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        # Q-Learningæ›´æ–°å…¬å¼
        current_q = self.q_table[state, action]
        
        if done:
            # å¦‚æœå›åˆç»“æŸï¼Œç›®æ ‡å€¼å°±æ˜¯å¥–åŠ±
            target = reward
        else:
            # å¦åˆ™ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹
            max_next_q = np.max(self.q_table[next_state])
            target = reward + self.discount_factor * max_next_q
        
        # æ›´æ–°Qå€¼
        self.q_table[state, action] = current_q + self.learning_rate * (target - current_q)
    
    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'q_table': self.q_table,
            'epsilon': self.epsilon,
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'success_rates': self.success_rates
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.q_table = model_data['q_table']
        self.epsilon = model_data['epsilon']
        self.learning_rate = model_data['learning_rate']
        self.discount_factor = model_data['discount_factor']
        self.episode_rewards = model_data.get('episode_rewards', [])
        self.episode_lengths = model_data.get('episode_lengths', [])
        self.success_rates = model_data.get('success_rates', [])

def train_qlearning(env_id: str = "FrozenLake-v1",
                   total_episodes: int = 5000,
                   learning_rate: float = 0.1,
                   discount_factor: float = 0.95,
                   epsilon: float = 1.0,
                   epsilon_decay: float = 0.995,
                   epsilon_min: float = 0.01,
                   eval_freq: int = 500,
                   seed: int = 42,
                   verbose: bool = True):
    """
    è®­ç»ƒQ-Learningæ™ºèƒ½ä½“
    
    Args:
        env_id: ç¯å¢ƒID
        total_episodes: æ€»è®­ç»ƒå›åˆæ•°
        learning_rate: å­¦ä¹ ç‡
        discount_factor: æŠ˜æ‰£å› å­
        epsilon: åˆå§‹æ¢ç´¢ç‡
        epsilon_decay: æ¢ç´¢ç‡è¡°å‡ç‡
        epsilon_min: æœ€å°æ¢ç´¢ç‡
        eval_freq: è¯„ä¼°é¢‘ç‡
        save_freq: ä¿å­˜é¢‘ç‡
        seed: éšæœºç§å­
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    np.random.seed(seed)
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(env_id, map_name="8x8", is_slippery=True)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = QLearningAgent(
        state_size=env.observation_space.n,
        action_size=env.action_space.n,
        learning_rate=learning_rate,
        discount_factor=discount_factor,
        epsilon=epsilon,
        epsilon_decay=epsilon_decay,
        epsilon_min=epsilon_min,
        seed=seed
    )
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®­ç»ƒæ¨¡å‹
    model_path = "frozenlake_qlearning_model.pkl"
    if os.path.exists(model_path):
        if verbose:
            print(f"ğŸ” å‘ç°é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        try:
            agent.load_model(model_path)
            if verbose:
                print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
        except Exception as e:
            if verbose:
                print(f"âš ï¸  åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œé‡æ–°å¼€å§‹è®­ç»ƒ")
    
    if verbose:
        print("ğŸš€ å¼€å§‹Q-Learningè®­ç»ƒ...")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°:")
        print(f"   â€¢ ç¯å¢ƒ: {env_id} (8x8)")
        print(f"   â€¢ æ€»å›åˆæ•°: {total_episodes:,}")
        print(f"   â€¢ å­¦ä¹ ç‡: {learning_rate}")
        print(f"   â€¢ æŠ˜æ‰£å› å­: {discount_factor}")
        print(f"   â€¢ åˆå§‹æ¢ç´¢ç‡: {epsilon}")
        print(f"   â€¢ æ¢ç´¢ç‡è¡°å‡: {epsilon_decay}")
        print(f"   â€¢ æœ€å°æ¢ç´¢ç‡: {epsilon_min}")
        print("=" * 60)
    
    start_time = time.time()
    
    for episode in range(total_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # æ›´æ–°Qè¡¨
            agent.update_q_table(state, action, reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            episode_reward += reward
            episode_length += 1
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        agent.episode_rewards.append(episode_reward)
        agent.episode_lengths.append(episode_length)
        
        # è®¡ç®—æˆåŠŸç‡
        recent_rewards = agent.episode_rewards[-100:]
        success_rate = sum(1 for r in recent_rewards if r > 0) / len(recent_rewards) * 100
        agent.success_rates.append(success_rate)
        
        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()
        
        # å®šæœŸè¾“å‡ºè¿›åº¦
        if verbose and (episode + 1) % eval_freq == 0:
            avg_reward = np.mean(recent_rewards)
            avg_length = np.mean(agent.episode_lengths[-100:])
            print(f"ğŸ“Š å›åˆ {episode + 1:,}/{total_episodes:,}: "
                  f"æˆåŠŸç‡={success_rate:.1f}%, "
                  f"å¹³å‡å¥–åŠ±={avg_reward:.3f}, "
                  f"å¹³å‡æ­¥æ•°={avg_length:.1f}, "
                  f"Îµ={agent.epsilon:.4f}")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if (episode + 1) % 1000 == 0:
            agent.save_model(f"frozenlake_qlearning_checkpoint_{episode + 1}.pkl")
            if verbose:
                print(f"ğŸ’¾ å·²ä¿å­˜checkpoint: frozenlake_qlearning_checkpoint_{episode + 1}.pkl")
    
    training_time = time.time() - start_time
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model(model_path)
    
    if verbose:
        final_success_rate = agent.success_rates[-1] if agent.success_rates else 0
        print(f"\nğŸ è®­ç»ƒå®Œæˆ!")
        print(f"â° è®­ç»ƒç”¨æ—¶: {training_time:.2f}ç§’")
        print(f"ğŸ“ˆ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%")
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    env.close()
    return agent

def evaluate_agent(agent: QLearningAgent, 
                  env_id: str = "FrozenLake-v1",
                  n_episodes: int = 100,
                  render: bool = False,
                  verbose: bool = True):
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    
    Args:
        agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
        env_id: ç¯å¢ƒID
        n_episodes: è¯„ä¼°å›åˆæ•°
        render: æ˜¯å¦æ¸²æŸ“
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    
    env = gym.make(env_id, map_name="8x8", is_slippery=True, 
                   render_mode='human' if render else None)
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    if verbose:
        print(f"\nğŸ® å¼€å§‹è¯„ä¼°æ™ºèƒ½ä½“ ({n_episodes}å›åˆ)...")
    
    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.select_action(state, training=False)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            episode_length += 1
            
            if render:
                time.sleep(0.1)  # å‡æ…¢é€Ÿåº¦ä»¥ä¾¿è§‚å¯Ÿ
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        if episode_reward > 0:
            success_count += 1
        
        if verbose and (episode + 1) % 20 == 0:
            current_success_rate = success_count / (episode + 1) * 100
            print(f"ğŸ“Š è¿›åº¦ {episode + 1}/{n_episodes}: æˆåŠŸç‡ {current_success_rate:.1f}%")
    
    env.close()
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    success_rate = success_count / n_episodes * 100
    avg_reward = np.mean(episode_rewards)
    avg_length = np.mean(episode_lengths)
    
    if verbose:
        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   â€¢ æˆåŠŸç‡: {success_rate:.1f}% ({success_count}/{n_episodes})")
        print(f"   â€¢ å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
        print(f"   â€¢ å¹³å‡æ­¥æ•°: {avg_length:.1f}")
    
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_length': avg_length,
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths
    }

def plot_training_progress(agent: QLearningAgent, save_path: str = "frozenlake_qlearning_progress.png"):
    """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾è¡¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('CleanRLé£æ ¼ Q-Learning è®­ç»ƒè¿›åº¦', fontsize=16, fontweight='bold')
    
    # æˆåŠŸç‡æ›²çº¿
    if agent.success_rates:
        axes[0, 0].plot(agent.success_rates, 'b-', linewidth=2)
        axes[0, 0].set_title('æˆåŠŸç‡å˜åŒ– (100å›åˆç§»åŠ¨å¹³å‡)')
        axes[0, 0].set_xlabel('å›åˆæ•°')
        axes[0, 0].set_ylabel('æˆåŠŸç‡ (%)')
        axes[0, 0].grid(True, alpha=0.3)
    
    # å›åˆå¥–åŠ±åˆ†å¸ƒ
    if agent.episode_rewards:
        axes[0, 1].hist(agent.episode_rewards, bins=3, alpha=0.7, color='green')
        axes[0, 1].set_title('å›åˆå¥–åŠ±åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('å¥–åŠ±')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].grid(True, alpha=0.3)
    
    # å›åˆé•¿åº¦å˜åŒ–
    if agent.episode_lengths:
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window_size = min(100, len(agent.episode_lengths) // 10)
        if window_size > 1:
            moving_avg = np.convolve(agent.episode_lengths, 
                                   np.ones(window_size)/window_size, mode='valid')
            axes[1, 0].plot(moving_avg, 'r-', linewidth=2, label=f'{window_size}å›åˆç§»åŠ¨å¹³å‡')
            axes[1, 0].legend()
        else:
            axes[1, 0].plot(agent.episode_lengths, 'r-', alpha=0.6)
        
        axes[1, 0].set_title('å›åˆæ­¥æ•°å˜åŒ–')
        axes[1, 0].set_xlabel('å›åˆæ•°')
        axes[1, 0].set_ylabel('æ­¥æ•°')
        axes[1, 0].grid(True, alpha=0.3)
    
    # è®­ç»ƒæ€»ç»“ä¿¡æ¯
    axes[1, 1].axis('off')
    final_success_rate = agent.success_rates[-1] if agent.success_rates else 0
    summary_text = f"""
    è®­ç»ƒæ€»ç»“:
    
    â€¢ æ€»å›åˆæ•°: {len(agent.episode_rewards):,}
    â€¢ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%
    â€¢ å¹³å‡å›åˆé•¿åº¦: {np.mean(agent.episode_lengths):.1f}æ­¥
    â€¢ æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.4f}
    â€¢ å­¦ä¹ ç‡: {agent.learning_rate}
    â€¢ æŠ˜æ‰£å› å­: {agent.discount_factor}
    
    ç®—æ³•: æœ´ç´ Q-Learning
    ç¯å¢ƒ: FrozenLake-v1 (8x8)
    å®ç°é£æ ¼: CleanRL
    """
    
    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,
                   fontsize=12, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    try:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“ˆ è®­ç»ƒè¿›åº¦å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
    
    plt.show()

def main():
    """ä¸»å‡½æ•° - ç®€æ´çš„Q-Learningæ¼”ç¤º"""
    
    print("=" * 60)
    print("ğŸ¯ æœ´ç´ Q-Learningç®—æ³•æ¼”ç¤º - FrozenLake 8x8")
    print("ä¸“æ³¨äºå±•ç¤ºQ-Learningçš„æ ¸å¿ƒåŸç†å’Œå®ç°")
    print("=" * 60)
    
    # è®­ç»ƒæ™ºèƒ½ä½“
    agent = train_qlearning(
        total_episodes=5000,
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=1.0,
        epsilon_decay=0.995,
        epsilon_min=0.01,
        eval_freq=500,
        seed=42,
        verbose=True
    )
    
    # ç»˜åˆ¶è®­ç»ƒè¿›åº¦
    plot_training_progress(agent)
    
    # è¯„ä¼°æ™ºèƒ½ä½“
    print("\n" + "="*50)
    eval_results = evaluate_agent(
        agent, 
        n_episodes=100, 
        render=False,
        verbose=True
    )
    
    # è¯¢é—®æ˜¯å¦è¿›è¡Œå¯è§†åŒ–æ¼”ç¤º
    try:
        response = input("\nğŸ® æ˜¯å¦è¿›è¡Œå¯è§†åŒ–æ¼”ç¤º? (y/n): ").lower().strip()
        if response in ['y', 'yes', 'æ˜¯']:
            print("ğŸ® å¼€å§‹å¯è§†åŒ–æ¼”ç¤º...")
            evaluate_agent(agent, n_episodes=5, render=True, verbose=False)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºç»“æŸ")
    except:
        print("ğŸ® è·³è¿‡å¯è§†åŒ–æ¼”ç¤ºï¼ˆéäº¤äº’ç¯å¢ƒï¼‰")
    
    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ!")

if __name__ == "__main__":
    main()
