import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import time
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random

# ==================== å¸¸é‡å®šä¹‰ ====================
class DQNConfig:
    """DQNç®—æ³•é…ç½®å¸¸é‡"""
    # ç¯å¢ƒé…ç½®
    ENV_NAME = 'FrozenLake-v1'
    MAP_NAME = "8x8"  # 8x8åœ°å›¾
    IS_SLIPPERY = False  # å…³é—­æ»‘å†°ï¼Œè®©æ™ºèƒ½ä½“å­¦ä¼šåŸºæœ¬ç­–ç•¥
    
    # ç½‘ç»œç»“æ„é…ç½®
    INPUT_SIZE = 64  # 8x8 = 64ä¸ªçŠ¶æ€
    HIDDEN_SIZE = 128  # éšè—å±‚å¤§å°
    OUTPUT_SIZE = 4   # 4ä¸ªåŠ¨ä½œ
    NUM_HIDDEN_LAYERS = 2  # éšè—å±‚æ•°é‡
    
    # å­¦ä¹ å‚æ•°
    LEARNING_RATE = 0.001
    GAMMA = 0.95  # æŠ˜æ‰£å› å­
    EPSILON_START = 1.0
    EPSILON_END = 0.01
    EPSILON_DECAY = 0.995
    
    # ç»éªŒå›æ”¾é…ç½®
    REPLAY_BUFFER_SIZE = 10000
    BATCH_SIZE = 32
    MIN_REPLAY_SIZE = 1000  # å¼€å§‹è®­ç»ƒå‰çš„æœ€å°ç»éªŒæ•°é‡
    
    # ç›®æ ‡ç½‘ç»œé…ç½®
    TARGET_UPDATE_FREQUENCY = 100  # æ¯100æ­¥æ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
    
    # è®­ç»ƒé…ç½®
    DEFAULT_TRAINING_EPISODES = 10000
    DEMO_EPISODES = 100
    MAX_STEPS_PER_EPISODE = 200
    
    # å¥–åŠ±å¡‘é€ 
    HOLE_PENALTY = -0.1
    SUCCESS_REWARD = 1.0
    STEP_PENALTY = -0.001
    
    # æ£€æŸ¥ç‚¹é…ç½®
    CHECKPOINT_INTERVAL = 500
    MODEL_FILE = 'frozen_lake_dqn_model.pkl'
    CHECKPOINT_FILE = 'frozenlake_dqn_checkpoint.pkl'
    PLOT_FILE = 'frozen_lake_dqn_progress.png'
    
    # è®¾å¤‡é…ç½®
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ˜¾ç¤ºé…ç½®
    LOG_INTERVAL = 100
    SEPARATOR_LENGTH = 80
    DEMO_SEPARATOR_LENGTH = 50
    ACTION_NAMES = ["å·¦", "ä¸‹", "å³", "ä¸Š"]

# ==================== DQNç½‘ç»œæ¨¡å‹ ====================
class DQNNetwork(nn.Module):
    """DQNç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size, output_size, num_hidden_layers=2):
        super(DQNNetwork, self).__init__()
        
        # è¾“å…¥å±‚
        self.input_layer = nn.Linear(input_size, hidden_size)
        
        # éšè—å±‚
        self.hidden_layers = nn.ModuleList()
        for _ in range(num_hidden_layers - 1):
            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_size, output_size)
        
        # Dropoutå±‚ï¼ˆå¯é€‰ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # è¾“å…¥å±‚ + ReLUæ¿€æ´»
        x = F.relu(self.input_layer(x))
        x = self.dropout(x)
        
        # éšè—å±‚ + ReLUæ¿€æ´»
        for hidden_layer in self.hidden_layers:
            x = F.relu(hidden_layer(x))
            x = self.dropout(x)
        
        # è¾“å‡ºå±‚ï¼ˆä¸æ¿€æ´»ï¼Œç›´æ¥è¾“å‡ºQå€¼ï¼‰
        x = self.output_layer(x)
        return x

# ==================== ç»éªŒå›æ”¾ç¼“å†²åŒº ====================
class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """ä»ç¼“å†²åŒºéšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

# ==================== DQNæ™ºèƒ½ä½“ ====================
class DQNAgent:
    """DQNæ™ºèƒ½ä½“"""
    
    def __init__(self, config):
        self.config = config
        self.device = config.DEVICE
        
        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQNNetwork(
            config.INPUT_SIZE, 
            config.HIDDEN_SIZE, 
            config.OUTPUT_SIZE,
            config.NUM_HIDDEN_LAYERS
        ).to(self.device)
        
        self.target_network = DQNNetwork(
            config.INPUT_SIZE, 
            config.HIDDEN_SIZE, 
            config.OUTPUT_SIZE,
            config.NUM_HIDDEN_LAYERS
        ).to(self.device)
        
        # å¤åˆ¶ä¸»ç½‘ç»œå‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config.LEARNING_RATE)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(config.REPLAY_BUFFER_SIZE)
        
        # æ¢ç´¢ç‡
        self.epsilon = config.EPSILON_START
        
        # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        self.step_count = 0
        
    def state_to_tensor(self, state):
        """å°†çŠ¶æ€è½¬æ¢ä¸ºone-hotå¼ é‡"""
        state_tensor = torch.zeros(self.config.INPUT_SIZE, dtype=torch.float32)
        state_tensor[state] = 1.0
        return state_tensor.unsqueeze(0).to(self.device)
    
    def select_action(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        if training and random.random() < self.epsilon:
            # éšæœºæ¢ç´¢
            return random.randint(0, self.config.OUTPUT_SIZE - 1)
        else:
            # è´ªå©ªé€‰æ‹©
            with torch.no_grad():
                state_tensor = self.state_to_tensor(state)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()
    
    def store_experience(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        state_tensor = self.state_to_tensor(state).squeeze(0).cpu().numpy()
        next_state_tensor = self.state_to_tensor(next_state).squeeze(0).cpu().numpy()
        
        self.replay_buffer.push(state_tensor, action, reward, next_state_tensor, done)
    
    def train(self):
        """è®­ç»ƒç½‘ç»œ"""
        if len(self.replay_buffer) < self.config.MIN_REPLAY_SIZE:
            return
        
        # ä»å›æ”¾ç¼“å†²åŒºé‡‡æ ·
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.config.BATCH_SIZE)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.config.GAMMA * next_q_values * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.step_count += 1
        if self.step_count % self.config.TARGET_UPDATE_FREQUENCY == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.config.EPSILON_END:
            self.epsilon *= self.config.EPSILON_DECAY
    
    def save_model(self, filepath, episode=None):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'step_count': self.step_count
        }
        if episode is not None:
            checkpoint['episode'] = episode
        torch.save(checkpoint, filepath)
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.step_count = checkpoint['step_count']

# ==================== è¾…åŠ©å‡½æ•° ====================
def create_environment(render=False):
    """åˆ›å»ºFrozenLakeç¯å¢ƒ"""
    try:
        env = gym.make(
            DQNConfig.ENV_NAME, 
            map_name=DQNConfig.MAP_NAME, 
            is_slippery=DQNConfig.IS_SLIPPERY, 
            render_mode='human' if render else None
        )
        return env
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¯å¢ƒå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…gymnasiumå’ŒFrozenLakeç¯å¢ƒ")
        return None

def shape_reward(reward, terminated, truncated):
    """å¥–åŠ±å¡‘é€ """
    shaped_reward = reward
    
    if terminated and reward == 0.0:  # æ‰å…¥å†°æ´
        shaped_reward += DQNConfig.HOLE_PENALTY
    elif not terminated and not truncated:  # æ­£å¸¸æ­¥éª¤
        shaped_reward += DQNConfig.STEP_PENALTY
    
    return shaped_reward

# ==================== ä¸»è®­ç»ƒå‡½æ•° ====================
def train_dqn(episodes, render=False, log_details=True, checkpoint_file=None):
    """
    è®­ç»ƒDQNæ™ºèƒ½ä½“
    
    Args:
        episodes: è®­ç»ƒå›åˆæ•°
        render: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–ç•Œé¢
        log_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    """
    if checkpoint_file is None:
        checkpoint_file = DQNConfig.CHECKPOINT_FILE
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = create_environment(render)
    if env is None:
        return
    
    agent = DQNAgent(DQNConfig)
    
    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_file):
        try:
            agent.load_model(checkpoint_file)
            print(f"âœ… åŠ è½½æ£€æŸ¥ç‚¹: æ­¥æ•°={agent.step_count}, Îµ={agent.epsilon:.4f}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}, ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # è®­ç»ƒç»Ÿè®¡
    rewards_per_episode = []
    success_count = 0
    
    if log_details:
        print("=" * DQNConfig.SEPARATOR_LENGTH)
        print("ğŸ§  DQN ç®—æ³•è®­ç»ƒå¼€å§‹")
        print(f"ğŸ“Š ç¯å¢ƒ: FrozenLake 8x8, æ€»å›åˆæ•°: {episodes}")
        print(f"ğŸ”§ è®¾å¤‡: {DQNConfig.DEVICE}")
        print(f"ğŸ§  ç½‘ç»œç»“æ„: {DQNConfig.INPUT_SIZE}â†’{DQNConfig.HIDDEN_SIZE}â†’{DQNConfig.OUTPUT_SIZE}")
        print(f"ğŸ“š å›æ”¾ç¼“å†²åŒº: {DQNConfig.REPLAY_BUFFER_SIZE}, æ‰¹æ¬¡å¤§å°: {DQNConfig.BATCH_SIZE}")
        print("=" * DQNConfig.SEPARATOR_LENGTH)
    else:
        print(f"ğŸ§  å¼€å§‹DQNè®­ç»ƒ: {episodes}å›åˆ, è®¾å¤‡: {DQNConfig.DEVICE}")
        print("ğŸ’¡ è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºè¿›åº¦ä¿¡æ¯...")
    
    for episode in range(episodes):
        state = env.reset()[0]
        episode_reward = 0
        episode_steps = 0
        
        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼ˆæ¯100å›åˆæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        if not log_details and (episode + 1) % 100 == 0:
            progress = (episode + 1) / episodes * 100
            print(f'\rğŸš€ è®­ç»ƒè¿›åº¦: {progress:.1f}% ({episode + 1}/{episodes})', end='', flush=True)
        
        while episode_steps < DQNConfig.MAX_STEPS_PER_EPISODE:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            # å¥–åŠ±å¡‘é€ 
            shaped_reward = shape_reward(reward, terminated, truncated)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_experience(state, action, shaped_reward, next_state, terminated or truncated)
            
            # è®­ç»ƒç½‘ç»œ
            agent.train()
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            episode_reward += reward
            episode_steps += 1
            
            if terminated or truncated:
                break
        
        # è®°å½•æˆåŠŸ
        if reward > 0:
            success_count += 1
        
        rewards_per_episode.append(episode_reward)
        
        # æ˜¾ç¤ºè¿›åº¦
        if log_details and (episode + 1) % DQNConfig.LOG_INTERVAL == 0:
            avg_reward = np.mean(rewards_per_episode[-DQNConfig.LOG_INTERVAL:])
            success_rate = success_count / (episode + 1) * 100
            print(f"å›åˆ {episode + 1}/{episodes}: "
                  f"å¹³å‡å¥–åŠ±={avg_reward:.3f}, æˆåŠŸç‡={success_rate:.1f}%, "
                  f"Îµ={agent.epsilon:.4f}, æ­¥æ•°={agent.step_count}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (episode + 1) % DQNConfig.CHECKPOINT_INTERVAL == 0:
            agent.save_model(checkpoint_file, episode + 1)
            if log_details:
                print(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {checkpoint_file}")
    
    env.close()
    
    # è®­ç»ƒå®Œæˆåæ¢è¡Œï¼Œè®©è¿›åº¦æ¡æ˜¾ç¤ºå®Œæ•´
    if not log_details:
        print()  # æ¢è¡Œ
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    if log_details:
        final_success_rate = success_count / episodes * 100
        avg_final_reward = np.mean(rewards_per_episode[-100:]) if len(rewards_per_episode) >= 100 else np.mean(rewards_per_episode)
        
        print("\n" + "=" * DQNConfig.SEPARATOR_LENGTH)
        print("ğŸ DQNè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   â€¢ æ€»å›åˆæ•°: {episodes}")
        print(f"   â€¢ æˆåŠŸç‡: {final_success_rate:.2f}%")
        print(f"   â€¢ æœ€å100å›åˆå¹³å‡å¥–åŠ±: {avg_final_reward:.3f}")
        print(f"   â€¢ æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.4f}")
        print(f"   â€¢ æ€»è®­ç»ƒæ­¥æ•°: {agent.step_count}")
        print("=" * DQNConfig.SEPARATOR_LENGTH)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model(DQNConfig.MODEL_FILE, episodes)
    if log_details:
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ° {DQNConfig.MODEL_FILE}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_progress(rewards_per_episode, log_details)
    
    return agent

def plot_training_progress(rewards_per_episode, log_details=True):
    """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
    plt.figure(figsize=(12, 8))
    
    # ç§»åŠ¨å¹³å‡å¥–åŠ±
    window_size = 100
    if len(rewards_per_episode) >= window_size:
        moving_avg = np.convolve(rewards_per_episode, np.ones(window_size)/window_size, mode='valid')
        plt.subplot(2, 1, 1)
        plt.plot(moving_avg)
        plt.title(f'DQNè®­ç»ƒè¿‡ç¨‹ - ç§»åŠ¨å¹³å‡å¥–åŠ± (çª—å£å¤§å°: {window_size})', fontsize=14)
        plt.xlabel('å›åˆæ•°')
        plt.ylabel('å¹³å‡å¥–åŠ±')
        plt.grid(True, alpha=0.3)
    
    # æˆåŠŸç‡æ›²çº¿
    success_rate = []
    success_count = 0
    for i, reward in enumerate(rewards_per_episode):
        if reward > 0:
            success_count += 1
        success_rate.append(success_count / (i + 1) * 100)
    
    plt.subplot(2, 1, 2)
    plt.plot(success_rate, 'r-', linewidth=2)
    plt.title('ç´¯ç§¯æˆåŠŸç‡å˜åŒ–', fontsize=14)
    plt.xlabel('å›åˆæ•°')
    plt.ylabel('æˆåŠŸç‡ (%)')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    try:
        plt.savefig(DQNConfig.PLOT_FILE, dpi=150, bbox_inches='tight')
        if log_details:
            print(f"ğŸ“ˆ è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ° {DQNConfig.PLOT_FILE}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
    
    plt.close()

def demo_trained_agent(episodes=100, render=True, log_details=True):
    """æ¼”ç¤ºè®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = create_environment(render)
    if env is None:
        return
    
    agent = DQNAgent(DQNConfig)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    try:
        agent.load_model(DQNConfig.MODEL_FILE)
        print(f"âœ… åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: æ­¥æ•°={agent.step_count}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ '{DQNConfig.MODEL_FILE}'")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼")
        return
    
    # è®¾ç½®æµ‹è¯•æ¨¡å¼ï¼ˆä¸æ¢ç´¢ï¼‰
    agent.epsilon = 0.0
    
    success_count = 0
    total_reward = 0
    
    if log_details:
        print("\n" + "=" * DQNConfig.DEMO_SEPARATOR_LENGTH)
        print("ğŸ® æ¼”ç¤ºè®­ç»ƒå¥½çš„DQNæ™ºèƒ½ä½“")
        print("=" * DQNConfig.DEMO_SEPARATOR_LENGTH)
    
    for episode in range(episodes):
        state = env.reset()[0]
        episode_reward = 0
        episode_steps = 0
        
        if log_details and episode < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªå›åˆçš„è¯¦ç»†ä¿¡æ¯
            print(f"\nğŸ“ æ¼”ç¤ºå›åˆ {episode + 1}")
        
        while episode_steps < DQNConfig.MAX_STEPS_PER_EPISODE:
            # é€‰æ‹©åŠ¨ä½œï¼ˆè´ªå©ªç­–ç•¥ï¼‰
            action = agent.select_action(state, training=False)
            
            if log_details and episode < 5:
                print(f"  æ­¥éª¤ {episode_steps + 1}: çŠ¶æ€ {state} â†’ åŠ¨ä½œ '{DQNConfig.ACTION_NAMES[action]}'")
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            state = next_state
            episode_reward += reward
            episode_steps += 1
            
            if terminated or truncated:
                break
        
        if reward > 0:
            success_count += 1
            if log_details and episode < 5:
                print(f"  ğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! å¥–åŠ±: {reward}")
        else:
            if log_details and episode < 5:
                print(f"  â„ï¸ æ‰å…¥å†°æ´æˆ–è¶…æ—¶")
        
        total_reward += episode_reward
        
        if log_details and episode < 5:
            print(f"  å›åˆæ€»ç»“: {episode_steps}æ­¥, å¥–åŠ±={episode_reward}")
    
    env.close()
    
    # æ¼”ç¤ºæ€»ç»“
    success_rate = success_count / episodes * 100
    avg_reward = total_reward / episodes
    
    if log_details:
        print(f"\nğŸ“Š æ¼”ç¤ºæ€»ç»“:")
        print(f"   â€¢ æ¼”ç¤ºå›åˆæ•°: {episodes}")
        print(f"   â€¢ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   â€¢ å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
        print("=" * DQNConfig.DEMO_SEPARATOR_LENGTH)

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ DQN ç®—æ³•æ¼”ç¤º")
    print("ğŸ’¡ æç¤º: render=True æ˜¾ç¤ºå¯è§†åŒ–ç•Œé¢ï¼Œrender=False åŠ å¿«è®­ç»ƒé€Ÿåº¦")
    print("ğŸ’¡ æç¤º: log_details=False å¯ä»¥å…³é—­è¯¦ç»†æ—¥å¿—")
    print()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„æ£€æŸ¥ç‚¹
    checkpoint_file = DQNConfig.CHECKPOINT_FILE
    if os.path.exists(checkpoint_file):
        try:
            checkpoint = torch.load(checkpoint_file, map_location=DQNConfig.DEVICE)
            print(f"ğŸ” å‘ç°ä¹‹å‰çš„æ£€æŸ¥ç‚¹: å·²è®­ç»ƒ {checkpoint['step_count']} æ­¥")
            print(f"   æ¢ç´¢ç‡: {checkpoint['epsilon']:.4f}")
            if 'episode' in checkpoint:
                print(f"   å·²è®­ç»ƒå›åˆ: {checkpoint['episode']}")
            
            # è¯¢é—®ç”¨æˆ·é€‰æ‹©
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. ç›´æ¥æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹")
            print("2. ç»§ç»­è®­ç»ƒæ›´å¤šå›åˆ")
            print("3. é‡æ–°å¼€å§‹è®­ç»ƒ")
            
            choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
            
            if choice == '1':
                print("âœ… ç›´æ¥è¿›å…¥æ¼”ç¤ºæ¨¡å¼...")
                # è·³è¿‡è®­ç»ƒï¼Œç›´æ¥è¿›å…¥æ¼”ç¤º
                pass
            elif choice == '2':
                print("âœ… ç»§ç»­è®­ç»ƒ...")
                try:
                    episodes = int(input(f"è¯·è¾“å…¥è¦è®­ç»ƒçš„å›åˆæ•° (é»˜è®¤ {DQNConfig.DEFAULT_TRAINING_EPISODES}): ") or DQNConfig.DEFAULT_TRAINING_EPISODES)
                    if episodes > 0:
                        print(f"ğŸ“Š å¼€å§‹è®­ç»ƒ {episodes} å›åˆ...")
                        train_dqn(episodes=episodes, 
                                 render=False, log_details=False, checkpoint_file=checkpoint_file)
                    else:
                        print("âŒ å›åˆæ•°å¿…é¡»å¤§äº0ï¼Œè¿›å…¥æ¼”ç¤ºæ¨¡å¼")
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼Œè¿›å…¥æ¼”ç¤ºæ¨¡å¼")
            elif choice == '3':
                print("ğŸ†• å¼€å§‹æ–°è®­ç»ƒ...")
                train_dqn(episodes=DQNConfig.DEFAULT_TRAINING_EPISODES, 
                         render=False, log_details=False)
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤è¿›å…¥æ¼”ç¤ºæ¨¡å¼")
                # é»˜è®¤è¿›å…¥æ¼”ç¤ºæ¨¡å¼
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}, é‡æ–°å¼€å§‹è®­ç»ƒ")
            train_dqn(episodes=DQNConfig.DEFAULT_TRAINING_EPISODES, 
                     render=False, log_details=False)
    else:
        print("ğŸ†• å¼€å§‹æ–°è®­ç»ƒ...")
        train_dqn(episodes=DQNConfig.DEFAULT_TRAINING_EPISODES, 
                 render=False, log_details=False)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼ç°åœ¨å¼€å§‹å¯è§†åŒ–æ¼”ç¤º...")
    input("æŒ‰Enteré”®å¼€å§‹æ¼”ç¤º...")
    
    # æ¼”ç¤ºè®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    try:
        demo_trained_agent(episodes=DQNConfig.DEMO_EPISODES, render=True, log_details=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºç»“æŸ")
